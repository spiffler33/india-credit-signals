# Data files — too large for git, regenerated by scrapers
data/raw/
# Exception: sourced dataset IS tracked (verified from SEBI disclosures)
!data/raw/rating_actions_sourced.csv
data/models/

# Dashboard export files — now tracked for Streamlit Cloud deployment (~2.2 MB total).
# Regenerate with: python -m src.signals.export_dashboard_data
# These are small enough to track (parquet is compressed) and Streamlit Cloud
# needs them in the repo since it can't run the export pipeline at deploy time.

# Large intermediate GDELT CSVs (reproducible from raw data + scripts)
# These range from 4-54MB each — too large for git, regenerated by pipeline
data/processed/gdelt_deduped.csv
data/processed/gdelt_for_labeling.csv
data/processed/gdelt_review_promoted.csv
data/processed/gdelt_triaged_*.csv
# Keep: label files, config samples, audit candidates (all <17MB)

# Python
*.pyc
__pycache__/
.venv/

# Environment secrets
.env

# Node.js (dashboard frontend)
node_modules/
dist/

# ML artifacts — model checkpoints can be GBs each
wandb/
*.ckpt
*.safetensors

# Reference repos — cloned for study, not our code
research/FinRLlama/
research/FinGPT/

# OS files
.DS_Store

# IDE
.vscode/
.idea/

# Claude Code local settings (machine-specific)
.claude/settings.local.json
