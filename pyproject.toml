# WHY THIS: Single source of truth for all project metadata and dependencies.
# uv reads this file to create a virtual environment and install everything.
# All Rosetta-era version pins removed — now using native ARM Python on M1.

[project]
name = "india-credit-signals"
version = "0.1.0"
description = "Fine-tuned LLM for extracting credit risk deterioration signals from news for Indian NBFCs"
readme = "PLAN.md"
requires-python = ">=3.12"
dependencies = [
    # --- HuggingFace ML ecosystem ---
    # transformers: The core library for loading and fine-tuning LLMs (like Bloomberg Terminal for models)
    "transformers>=4.48.0",
    # peft: Parameter-Efficient Fine-Tuning — implements LoRA, QLoRA, etc.
    "peft>=0.14.0",
    # datasets: HuggingFace's data loading/processing library (standardized data pipeline)
    "datasets>=3.0.0",
    # accelerate: Handles multi-GPU training, mixed precision — abstracts hardware complexity
    "accelerate>=1.0.0",
    # numpy: No longer pinned — ARM torch works with numpy 2.x
    "numpy>=2.0",
    # --- Data & Scraping ---
    # httpx: Modern async HTTP client (better than requests — supports async for parallel scraping)
    "httpx>=0.27.0",
    # tenacity: Retry logic for flaky network requests (scrapers WILL fail, this handles retries)
    "tenacity>=8.2.0",
    # pandas: Data manipulation (you know this from finance — DataFrames)
    "pandas>=2.2.0",
    # polars: Faster DataFrame library for large datasets (10-100x faster than pandas on big data)
    # No [rtcompat] needed — ARM Python has native wheels with full SIMD support
    "polars>=1.0.0",

    # --- API & Serving ---
    # fastapi: Modern Python web framework for the API backend
    "fastapi>=0.115.0",
    # uvicorn: ASGI server that runs FastAPI (like the engine under the API hood)
    "uvicorn>=0.29.0",

    # --- Config & Logging ---
    # pyyaml: Reads YAML config files (human-readable config, not hardcoded values)
    "pyyaml>=6.0",
    # loguru: Better logging than Python's built-in (structured, colored, easy to use)
    "loguru>=0.7.0",

    # --- Utilities ---
    # torch: PyTorch — the deep learning framework everything else is built on
    # No pin needed — latest torch has excellent M1 MPS support
    "torch>=2.5.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
    "ruff>=0.4.0",
    "pyright>=1.1.360",
]
# GPU-only deps — install on Colab/Lambda with: uv sync --extra gpu
gpu = [
    # bitsandbytes: Quantization for QLoRA (compresses 28GB model to ~5GB)
    "bitsandbytes>=0.43.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# WHY THIS: Our code lives in src/ not india_credit_signals/.
# This tells the build tool where to find our Python packages.
[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.ruff]
line-length = 100
target-version = "py311"

[tool.pyright]
pythonVersion = "3.11"
typeCheckingMode = "basic"
