{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2.2 â€” LoRA Fine-Tuning (Qwen 2.5-7B â†’ Credit Signal Extraction)\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads Qwen 2.5-7B-Instruct in 4-bit QLoRA configuration\n",
    "2. Applies LoRA adapters (r=16, targeting attention + MLP layers)\n",
    "3. Fine-tunes on 9,591 training examples using TRL's SFTTrainer\n",
    "4. Evaluates on validation set (2,247 examples, 2022-01 to 2023-06)\n",
    "5. Evaluates on test set (2,133 examples, 2023-07 to 2024-12)\n",
    "6. Evaluates on entity holdout (3,303 examples: DHFL, Reliance Capital, Cholamandalam)\n",
    "\n",
    "**Why LoRA instead of full fine-tuning:**\n",
    "Full fine-tuning updates all 7.6B parameters â€” needs 4x A100 GPUs (~$50/hr) and\n",
    "risks catastrophic forgetting on our small dataset (9.6K examples). LoRA inserts\n",
    "small trainable matrices (~33M new params, <0.5% of total) into attention and MLP\n",
    "layers. Original weights stay frozen. Result: trains on a single T4 in ~45-60 min,\n",
    "adapter weighs ~33MB instead of ~14GB, and the model retains its general language\n",
    "ability while learning our structured credit output format.\n",
    "\n",
    "**Phase 2.1 baseline (0% parse rate):**\n",
    "The base Qwen 2.5-7B writes free-form analyst paragraphs â€” zero format compliance.\n",
    "It understands credit concepts (NPAs, downgrades, rating agencies) from pre-training,\n",
    "but has never seen our `CREDIT_RELEVANT: / DIRECTION: / END` structured format.\n",
    "This training teaches the format. Any parse rate >80% is a success.\n",
    "\n",
    "**Time estimate:** ~45-60 min training on T4, ~20-30 min on A100. Eval adds ~15-30 min.\n",
    "\n",
    "**Data location:** `drive/MyDrive/india-credit-signals/data/processed/`\n",
    "**Model output:** `drive/MyDrive/india-credit-signals/data/models/qwen-credit-lora/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup â€” GPU, Packages, Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ Step 1a: Verify GPU allocation\n# T4 (15GB VRAM) is fine for QLoRA training of 7B models.\n# A100 (40GB) will be faster but not required.\n# With 4-bit quantized base + fp16 LoRA adapters, peak VRAM ~10-12GB.\n# ============================================================\n!nvidia-smi\n\nimport torch\nprint(f\"\\nPyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n    # ğŸ“ bf16 is better than fp16 for training (wider dynamic range, fewer inf/nan)\n    # but only Ampere+ GPUs (A100, A10G) support it. T4 is Turing â†’ fp16 only.\n    BF16_OK = torch.cuda.get_device_capability()[0] >= 8\n    print(f\"bf16 support: {BF16_OK} ({'Ampere+' if BF16_OK else 'Turing â€” using fp16'})\")\nelse:\n    raise RuntimeError(\"No GPU! Go to Runtime â†’ Change runtime type â†’ select GPU\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ“ Step 1b: Install dependencies\n",
    "# pip (not uv) because Colab's pre-built environment conflicts with uv.\n",
    "#\n",
    "# New packages vs Phase 2.1:\n",
    "#   peft: HuggingFace's LoRA/adapter library (creates the small trainable matrices)\n",
    "#   trl: Transformer Reinforcement Learning library â€” its SFTTrainer handles:\n",
    "#        - Chat template formatting (Qwen's <|im_start|>/<|im_end|> tokens)\n",
    "#        - Prompt masking (loss only on assistant response, not instruction/input)\n",
    "#        - Mixed precision training (fp16/bf16 automatically)\n",
    "#   datasets: HuggingFace datasets for data loading with SFTTrainer\n",
    "# ============================================================\n",
    "!pip install -q transformers>=4.48.0 accelerate>=1.0.0 bitsandbytes>=0.45.0 \\\n",
    "    peft>=0.14.0 trl>=0.15.0 datasets>=3.0.0 tqdm\n",
    "\n",
    "import transformers, accelerate, bitsandbytes, peft, trl, datasets as ds_lib\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"accelerate:   {accelerate.__version__}\")\n",
    "print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
    "print(f\"peft:         {peft.__version__}\")\n",
    "print(f\"trl:          {trl.__version__}\")\n",
    "print(f\"datasets:     {ds_lib.__version__}\")\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ“ Step 1c: Mount Google Drive and verify data files\n",
    "# Drive is the bridge: local Mac â†’ Drive (upload) â†’ Colab (train)\n",
    "# Trained adapters save back to Drive â†’ download to local for git\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DATA_DIR = \"/content/drive/MyDrive/india-credit-signals/data/processed\"\n",
    "MODEL_DIR = \"/content/drive/MyDrive/india-credit-signals/data/models/qwen-credit-lora\"\n",
    "assert os.path.isdir(DATA_DIR), f\"Data directory not found: {DATA_DIR}\\nUpload data to Google Drive first.\"\n",
    "\n",
    "# Create model output directory\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Verify all required files\n",
    "required_files = [\"train.jsonl\", \"val.jsonl\", \"test.jsonl\", \"entity_holdout.jsonl\"]\n",
    "for fname in required_files:\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        with open(fpath) as f:\n",
    "            n = sum(1 for _ in f)\n",
    "        print(f\"  âœ… {fname}: {n:,d} examples\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"  âŒ {fname}: NOT FOUND at {fpath}\")\n",
    "\n",
    "print(f\"\\nâœ… Data: {DATA_DIR}\")\n",
    "print(f\"âœ… Model output: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Base Model in 4-bit QLoRA Configuration\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ğŸ“ CONCEPT: QLoRA (Quantized LoRA)                          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ QLoRA combines two tricks:                                   â”‚\n",
    "â”‚ 1. Base model weights compressed to 4-bit (NF4 format)      â”‚\n",
    "â”‚    â†’ reduces 14GB model to ~5GB in VRAM                     â”‚\n",
    "â”‚ 2. LoRA adapter weights stay in full precision (fp16/bf16)  â”‚\n",
    "â”‚    â†’ only these ~33M params get gradient updates            â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ During forward pass: 4-bit weights get dequantized to fp16  â”‚\n",
    "â”‚ on the fly. During backward pass: only LoRA gradients flow. â”‚\n",
    "â”‚ Base weights never update.                                   â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ Result: train a 7B model on a single T4 (15GB VRAM) for     â”‚\n",
    "â”‚ ~$0 on Colab Pro. Without QLoRA you'd need â‰¥40GB VRAM.     â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ Paper: https://arxiv.org/abs/2305.14314                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# ğŸ“ Same BitsAndBytes config as Phase 2.1 â€” 4-bit NF4 with double quantization.\n",
    "# NF4 maps the 16 possible 4-bit values to the most common neural network weight\n",
    "# values (which follow a normal distribution), minimizing quantization error.\n",
    "# double_quant: quantizes the quantization scale factors themselves â†’ saves ~0.4GB.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if BF16_OK else torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} in 4-bit...\")\n",
    "t0 = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "load_time = time.time() - t0\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "gpu_mem_used = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "print(f\"âœ… Loaded in {load_time:.1f}s\")\n",
    "print(f\"   Parameters: {param_count / 1e9:.2f}B\")\n",
    "print(f\"   GPU memory used: {gpu_mem_used:.1f} GB\")\n",
    "print(f\"   Quantization: 4-bit NF4 + double quant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Tokenizer Configuration\n",
    "\n",
    "ğŸ“ **Qwen tokenizer gotchas â€” three things that MUST be set correctly:**\n",
    "\n",
    "1. **`pad_token â‰  eos_token`**: If pad and eos are the same token ID, the loss\n",
    "   function can't distinguish \"this is padding (ignore)\" from \"this is the real\n",
    "   end of the response (learn to stop here)\". Result: model either never stops\n",
    "   generating or stops randomly mid-output.\n",
    "\n",
    "2. **`padding_side=\"right\"`**: For causal LMs, pad tokens go at the END (right)\n",
    "   of the sequence, not the beginning. Left-padding shifts all positions and\n",
    "   confuses the positional embeddings.\n",
    "\n",
    "3. **No `bos_token` override**: Qwen 2.5 doesn't use a beginning-of-sequence\n",
    "   token. If you set one, you get a mysterious duplicate token at the start\n",
    "   of every generation (\"double-bot\" artifact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ğŸ“ Qwen 2.5 special tokens:\n",
    "#   <|endoftext|> (ID 151643) â€” general end-of-text marker\n",
    "#   <|im_end|> (ID 151645) â€” end of a chat turn (instruction/user/assistant)\n",
    "#   <|im_start|> (ID 151644) â€” start of a chat turn\n",
    "#\n",
    "# We set pad_token to <|endoftext|> and keep eos_token as <|im_end|>.\n",
    "# This way: padding is masked by pad_token_id, and the model learns to\n",
    "# stop generating when it outputs <|im_end|> (the real end signal).\n",
    "tokenizer.pad_token = \"<|endoftext|>\"       # ID 151643 â€” used for padding\n",
    "tokenizer.pad_token_id = 151643\n",
    "# eos_token stays as <|im_end|> (ID 151645) â€” Qwen's default\n",
    "tokenizer.padding_side = \"right\"              # ğŸ“ Causal LMs need right-padding\n",
    "\n",
    "print(f\"pad_token: {tokenizer.pad_token} (ID {tokenizer.pad_token_id})\")\n",
    "print(f\"eos_token: {tokenizer.eos_token} (ID {tokenizer.eos_token_id})\")\n",
    "print(f\"padding_side: {tokenizer.padding_side}\")\n",
    "assert tokenizer.pad_token_id != tokenizer.eos_token_id, \\\n",
    "    \"pad_token must differ from eos_token to prevent loss masking corruption!\"\n",
    "print(\"âœ… Tokenizer configured (pad â‰  eos verified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Load & Format Training Data\n",
    "\n",
    "ğŸ“ **Data conversion: instruction/input/output â†’ messages format**\n",
    "\n",
    "Our training JSONL has three fields per example:\n",
    "```json\n",
    "{\"instruction\": \"Assess whether...\", \"input\": \"Entity: DHFL\\nDate: ...\", \"output\": \"CREDIT_RELEVANT: Yes\\n...\"}\n",
    "```\n",
    "\n",
    "SFTTrainer expects a `messages` list with `system`/`user`/`assistant` roles:\n",
    "```json\n",
    "{\"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"Assess whether...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Entity: DHFL\\nDate: ...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"CREDIT_RELEVANT: Yes\\n...\"}\n",
    "]}\n",
    "```\n",
    "\n",
    "SFTTrainer then applies Qwen's chat template (`<|im_start|>system\\n...\\n<|im_end|>\\n...`)\n",
    "and uses `assistant_only_loss=True` to mask the instruction+input tokens from the loss.\n",
    "The model only learns to predict the assistant's response â€” not to parrot back the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_jsonl(path):\n",
    "    \"\"\"Load JSONL file into list of dicts.\"\"\"\n",
    "    records = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "def to_messages(example):\n",
    "    \"\"\"Convert instruction/input/output â†’ messages format for SFTTrainer.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Load raw JSONL\n",
    "train_raw = load_jsonl(os.path.join(DATA_DIR, \"train.jsonl\"))\n",
    "val_raw = load_jsonl(os.path.join(DATA_DIR, \"val.jsonl\"))\n",
    "\n",
    "# Convert to messages format\n",
    "train_messages = [to_messages(ex) for ex in train_raw]\n",
    "val_messages = [to_messages(ex) for ex in val_raw]\n",
    "\n",
    "# Create HuggingFace Datasets\n",
    "train_dataset = Dataset.from_list(train_messages)\n",
    "val_dataset = Dataset.from_list(val_messages)\n",
    "\n",
    "# ğŸ“ Show training data statistics before training (CLAUDE.md rule: never train\n",
    "# without showing data stats first)\n",
    "cr_yes_train = sum(1 for ex in train_raw if ex[\"output\"].startswith(\"CREDIT_RELEVANT: Yes\"))\n",
    "cr_no_train = len(train_raw) - cr_yes_train\n",
    "det_train = sum(1 for ex in train_raw if \"DIRECTION: Deterioration\" in ex[\"output\"])\n",
    "imp_train = sum(1 for ex in train_raw if \"DIRECTION: Improvement\" in ex[\"output\"])\n",
    "\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚ Training Data Statistics                         â”‚\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(f\"â”‚ Train examples:    {len(train_raw):>6,d}                      â”‚\")\n",
    "print(f\"â”‚ Val examples:      {len(val_raw):>6,d}                      â”‚\")\n",
    "print(f\"â”‚ Credit-relevant:   {cr_yes_train:>6,d}  ({cr_yes_train/len(train_raw)*100:.1f}%)            â”‚\")\n",
    "print(f\"â”‚ Not relevant:      {cr_no_train:>6,d}  ({cr_no_train/len(train_raw)*100:.1f}%)            â”‚\")\n",
    "print(f\"â”‚ Deterioration:     {det_train:>6,d}  ({det_train/len(train_raw)*100:.1f}%)            â”‚\")\n",
    "print(f\"â”‚ Improvement:       {imp_train:>6,d}  ({imp_train/len(train_raw)*100:.1f}%)            â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "# Show one formatted example\n",
    "print(\"\\nSample messages format (first example):\")\n",
    "for msg in train_messages[0][\"messages\"]:\n",
    "    role = msg[\"role\"]\n",
    "    content = msg[\"content\"][:120] + \"...\" if len(msg[\"content\"]) > 120 else msg[\"content\"]\n",
    "    print(f\"  [{role:>9s}] {content}\")\n",
    "\n",
    "# ğŸ“ Check token lengths to set max_seq_length\n",
    "# We need to know the longest example to set truncation correctly.\n",
    "# SFTTrainer will apply the chat template, so we measure AFTER template application.\n",
    "sample_lengths = []\n",
    "for ex in train_messages[:200]:  # Check first 200 for speed\n",
    "    text = tokenizer.apply_chat_template(ex[\"messages\"], tokenize=False)\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    sample_lengths.append(tokens[\"input_ids\"].shape[1])\n",
    "\n",
    "print(f\"\\nToken length stats (first 200 examples):\")\n",
    "print(f\"  Min: {min(sample_lengths)}, Max: {max(sample_lengths)}, \")\n",
    "print(f\"  Mean: {sum(sample_lengths)/len(sample_lengths):.0f}, \")\n",
    "print(f\"  P95: {sorted(sample_lengths)[int(0.95*len(sample_lengths))]}, \")\n",
    "print(f\"  P99: {sorted(sample_lengths)[int(0.99*len(sample_lengths))]}\")\n",
    "print(f\"\\nâœ… Data loaded and formatted: {len(train_dataset):,d} train, {len(val_dataset):,d} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: LoRA Configuration\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ğŸ“ CONCEPT: LoRA (Low-Rank Adaptation)                      â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ Instead of updating all 7.6B parameters, LoRA inserts small â”‚\n",
    "â”‚ trainable matrices into selected layers. For each target     â”‚\n",
    "â”‚ layer W (e.g., q_proj with shape 3584Ã—3584), LoRA adds:    â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚   W' = W + B Ã— A    where A is (r Ã— 3584) and B is (3584 Ã— r) â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ With r=16: each adapter pair is (16Ã—3584) + (3584Ã—16) =     â”‚\n",
    "â”‚ 114,688 params instead of 12.8M. Across 5 target modules   â”‚\n",
    "â”‚ Ã— 28 layers = ~33M new params total (<0.5% of 7.6B).       â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ alpha=32 scales the adapter output: effective lr per layer  â”‚\n",
    "â”‚ is alpha/r = 32/16 = 2x. Higher alpha â†’ more adapter       â”‚\n",
    "â”‚ influence. We use 2x because our task (format learning) is  â”‚\n",
    "â”‚ relatively easy â€” we don't want to overpower base knowledge.â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ Think of it as: teaching a fluent English speaker credit-   â”‚\n",
    "â”‚ specific jargon. You don't retrain their entire language.   â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ Paper: https://arxiv.org/abs/2106.09685                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# ğŸ“ DECISION: LoRA target modules â€” 5 of 7 available in Qwen 2.5\n",
    "#   Qwen 2.5 transformer layers have:\n",
    "#     Attention: q_proj, k_proj, v_proj, o_proj\n",
    "#     MLP: gate_proj, up_proj, down_proj\n",
    "#\n",
    "#   We target: q_proj, v_proj (attention patterns) + gate_proj, up_proj, down_proj (MLP)\n",
    "#   We skip: k_proj, o_proj (attention)\n",
    "#\n",
    "#   PRO: MLP layers control vocabulary/output distribution â€” critical for learning\n",
    "#        our structured format (CREDIT_RELEVANT: Yes/No, specific vocab words)\n",
    "#   PRO: q_proj + v_proj = standard LoRA recipe, proven on many models\n",
    "#   CON: More target modules = more params = slightly higher VRAM\n",
    "#   RISK: If VRAM overflows, drop MLP modules first (attention is more important)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                              # ğŸ“ Rank 16: ~33M trainable params\n",
    "    lora_alpha=32,                     # ğŸ“ alpha/r = 2x scaling factor\n",
    "    lora_dropout=0.05,                 # ğŸ“ Light dropout to prevent overfitting on 9.6K examples\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\",            # Attention: what to attend to\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",  # MLP: output vocabulary/format\n",
    "    ],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    bias=\"none\",                       # ğŸ“ Standard: don't train bias terms\n",
    ")\n",
    "\n",
    "# Apply LoRA to the quantized base model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameter summary\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Verify VRAM after LoRA application\n",
    "gpu_mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"\\nGPU memory after LoRA: {gpu_mem_after:.1f} GB\")\n",
    "print(f\"Headroom for training: {gpu_mem - gpu_mem_after:.1f} GB\")\n",
    "print(\"âœ… LoRA adapters applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Training Configuration\n",
    "\n",
    "ğŸ“ **DECISION: Using TRL's SFTTrainer instead of a custom training loop**\n",
    "- PRO: Handles Qwen chat template application automatically\n",
    "- PRO: `assistant_only_loss=True` â†’ loss computed only on assistant tokens (our output)\n",
    "  The model doesn't waste gradient updates learning to reproduce the instruction/article\n",
    "- PRO: Integrates with HuggingFace's Trainer (checkpointing, logging, eval)\n",
    "- CON: Less control than a custom loop (fine for our use case)\n",
    "- RISK: TRL versions can have breaking changes â€” we pin â‰¥0.15.0\n",
    "\n",
    "ğŸ“ **DECISION: lr=5e-4 (not 2e-4 as originally planned)**\n",
    "- QLoRA typically needs higher lr than full fine-tuning because gradients pass\n",
    "  through quantized weights with reduced precision\n",
    "- 5e-4 is the QLoRA paper's recommended starting point\n",
    "- With cosine schedule + 100 warmup steps, the effective lr ramps up gently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTConfig, SFTTrainer\n\n# ğŸ“ Training hyperparameters â€” annotated with WHY for each choice\ntraining_args = SFTConfig(\n    # --- Output ---\n    output_dir=\"/content/lora_checkpoints\",  # ğŸ“ Local Colab disk (fast), final model goes to Drive\n\n    # --- Training schedule ---\n    num_train_epochs=3,                      # ğŸ“ 3 passes over 9.6K examples. More epochs risk\n                                             #     overfitting on our small dataset.\n    per_device_train_batch_size=4,           # ğŸ“ 4 examples per GPU step\n    gradient_accumulation_steps=4,           # ğŸ“ Effective batch = 4 Ã— 4 = 16. Larger effective\n                                             #     batch â†’ smoother gradients â†’ stable training.\n                                             #     Tradeoff: fewer weight updates per epoch.\n\n    # --- Learning rate ---\n    learning_rate=5e-4,                      # ğŸ“ QLoRA sweet spot. Higher than standard fine-tune\n                                             #     (2e-4) because quantized gradients need more push.\n    lr_scheduler_type=\"cosine\",              # ğŸ“ Cosine decay: high lr early (learn fast) â†’ low lr\n                                             #     late (fine-tune carefully). Better than linear decay.\n    warmup_steps=100,                        # ğŸ“ First 100 steps: lr ramps from 0 â†’ 5e-4.\n                                             #     Prevents early gradient explosions on fresh LoRA.\n\n    # --- Precision ---\n    bf16=BF16_OK,                            # ğŸ“ Use bf16 on Ampere+ (A100), fp16 on Turing (T4)\n    fp16=not BF16_OK,\n\n    # --- Evaluation ---\n    eval_strategy=\"steps\",\n    eval_steps=500,                          # ğŸ“ Eval every 500 steps (~1/3 epoch). Catches\n                                             #     overfitting early without slowing training.\n    per_device_eval_batch_size=4,\n\n    # --- Checkpointing ---\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=3,                      # ğŸ“ Keep only 3 best checkpoints (disk space)\n    load_best_model_at_end=True,             # ğŸ“ After training, roll back to best val loss\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,                 # ğŸ“ Lower loss = better\n\n    # --- Logging ---\n    logging_steps=50,                        # ğŸ“ Log every 50 steps for training curve visibility\n    report_to=\"none\",                        # ğŸ“ No W&B/TensorBoard â€” keep it simple for now\n\n    # --- SFT-specific ---\n    # NOTE: max_seq_length is NOT a SFTConfig param â€” it goes in SFTTrainer directly.\n    dataset_text_field=None,                 # ğŸ“ We use messages format, not raw text\n\n    # --- Memory optimization ---\n    gradient_checkpointing=True,             # ğŸ“ Trades ~30% speed for ~40% VRAM savings.\n                                             #     Recomputes activations during backward pass\n                                             #     instead of storing them all. Essential for T4.\n    optim=\"adamw_torch_fused\",               # ğŸ“ Fused AdamW: single kernel for param update.\n                                             #     Faster + less memory than separate operations.\n)\n\n# ğŸ“ Enable gradient checkpointing on the model\n# This must be called before creating the trainer, and requires\n# use_reentrant=False for compatibility with LoRA.\nmodel.gradient_checkpointing_enable(\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n\n# Calculate training steps\ntotal_steps = (len(train_dataset) // (training_args.per_device_train_batch_size\n               * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\nprint(f\"\\nğŸ“ Training plan:\")\nprint(f\"   Examples: {len(train_dataset):,d}\")\nprint(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   Steps per epoch: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\nprint(f\"   Total steps: ~{total_steps:,d}\")\nprint(f\"   Eval every: {training_args.eval_steps} steps\")\nprint(f\"   Max seq length: 2048 tokens\")\nprint(f\"   Precision: {'bf16' if BF16_OK else 'fp16'}\")\nprint(f\"   Gradient checkpointing: ON\")\nprint(f\"âœ… Training config ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Train!\n",
    "\n",
    "ğŸ“ **What to watch during training:**\n",
    "- `train_loss` should decrease steadily across steps\n",
    "- `eval_loss` should decrease then flatten (if it starts INCREASING, that's overfitting)\n",
    "- If eval_loss increases for 2+ eval points â†’ training has gone too far\n",
    "  (`load_best_model_at_end=True` handles this automatically)\n",
    "- Total time: ~45-60 min on T4, ~20-30 min on A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“ SFTTrainer wraps HuggingFace Trainer with SFT-specific features:\n# - Applies Qwen's chat template to messages automatically\n# - assistant_only_loss: only computes loss on assistant (output) tokens\n#   The model learns to GENERATE our structured format, not to REPEAT\n#   the instruction and article back. Without this, ~80% of gradient\n#   updates would be wasted on the long input text.\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    processing_class=tokenizer,\n    max_seq_length=2048,                     # ğŸ“ Covers all our examples (longest ~1,200 tokens).\n                                             #     2048 gives headroom without wasting memory.\n                                             #     This param belongs on SFTTrainer, NOT SFTConfig.\n)\n\nprint(f\"Starting training...\")\nprint(f\"  Train: {len(train_dataset):,d} examples\")\nprint(f\"  Val: {len(val_dataset):,d} examples\")\nprint(f\"  Watch eval_loss â€” if it starts increasing, the model is overfitting.\")\nprint()\n\ntrain_result = trainer.train()\n\n# Print training summary\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\" * 70)\nmetrics = train_result.metrics\nprint(f\"  Total steps:      {metrics.get('total_flos', 'N/A')}\")\nprint(f\"  Training time:    {metrics.get('train_runtime', 0) / 60:.1f} min\")\nprint(f\"  Final train loss: {metrics.get('train_loss', 'N/A'):.4f}\")\nprint(f\"  Samples/sec:      {metrics.get('train_samples_per_second', 0):.1f}\")\nprint()\n\n# ğŸ“ Save only the LoRA adapters (not the full model)\n# The adapter is ~33MB. The base model is ~14GB. We save the adapter\n# and reload the base model + adapter at inference time.\n# Saving to Drive so it persists after Colab session ends.\nprint(f\"Saving adapters to {MODEL_DIR}...\")\ntrainer.save_model(MODEL_DIR)\ntokenizer.save_pretrained(MODEL_DIR)\n\n# Verify save\nadapter_files = os.listdir(MODEL_DIR)\nadapter_size = sum(os.path.getsize(os.path.join(MODEL_DIR, f))\n                   for f in adapter_files if os.path.isfile(os.path.join(MODEL_DIR, f)))\nprint(f\"  Saved {len(adapter_files)} files ({adapter_size / 1e6:.1f} MB)\")\nprint(f\"  Files: {adapter_files}\")\nprint(f\"âœ… Adapters saved to Drive\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Evaluation â€” Parse Rate + Field Accuracy on Validation Set\n",
    "\n",
    "ğŸ“ **This is where we see if training worked.**\n",
    "\n",
    "Phase 2.1 baseline: **0% parse rate** â€” base model writes free-form paragraphs.\n",
    "Target: **>80% parse rate** â€” model produces our structured format correctly.\n",
    "\n",
    "We evaluate on 500 validation examples (from 2022-01 to 2023-06).\n",
    "These are examples the model never saw during training but from a similar time period.\n",
    "This is a fair test of format learning, not generalization to future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRICT OUTPUT PARSER (copied from src/data/parse_training_output.py)\n",
    "# ğŸ“ SOURCE OF TRUTH: src/data/parse_training_output.py\n",
    "# Also in: src/training/evaluate.py, notebooks/phase2_1_base_model_eval.ipynb\n",
    "# If you change the parser, update the source file first.\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass, field as dataclass_field\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "VOCAB = {\n",
    "    \"credit_relevant\": {\"Yes\", \"No\"},\n",
    "    \"direction\": {\"Deterioration\", \"Improvement\", \"Neutral\"},\n",
    "    \"signal_type\": {\n",
    "        \"liquidity\", \"asset_quality\", \"regulatory\", \"contagion\",\n",
    "        \"governance\", \"funding\", \"operational\", \"other\",\n",
    "    },\n",
    "    \"sector_wide\": {\"Yes\", \"No\"},\n",
    "    \"confidence\": {\"Low\", \"Medium\", \"High\"},\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParsedOutput:\n",
    "    credit_relevant: bool = False\n",
    "    direction: str = \"\"\n",
    "    signal_type: str = \"\"\n",
    "    sector_wide: bool = False\n",
    "    confidence: str = \"\"\n",
    "    reasoning: str = \"\"\n",
    "    has_end_token: bool = False\n",
    "    parse_ok: bool = False\n",
    "    error_field: str = \"\"\n",
    "    error_detail: str = \"\"\n",
    "\n",
    "\n",
    "def parse_model_output(text, vocab=None):\n",
    "    if vocab is None:\n",
    "        vocab = VOCAB\n",
    "    result = ParsedOutput()\n",
    "    text = text.strip()\n",
    "\n",
    "    if text.endswith(\"END\"):\n",
    "        result.has_end_token = True\n",
    "        text = text[:-3].strip()\n",
    "    else:\n",
    "        result.error_field = \"END\"\n",
    "        result.error_detail = \"Missing END token\"\n",
    "\n",
    "    fields = {}\n",
    "    current_key = None\n",
    "    current_val = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        matched = False\n",
    "        for field_name in (\"CREDIT_RELEVANT\", \"DIRECTION\", \"SIGNAL_TYPE\",\n",
    "                           \"SECTOR_WIDE\", \"CONFIDENCE\", \"REASONING\"):\n",
    "            prefix = field_name + \":\"\n",
    "            if line.upper().startswith(prefix.upper()):\n",
    "                if current_key is not None:\n",
    "                    fields[current_key] = current_val.strip()\n",
    "                current_key = field_name\n",
    "                current_val = line[len(prefix):].strip()\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched and current_key is not None:\n",
    "            current_val += \" \" + line\n",
    "    if current_key is not None:\n",
    "        fields[current_key] = current_val.strip()\n",
    "\n",
    "    cr_val = fields.get(\"CREDIT_RELEVANT\", \"\").strip()\n",
    "    if not cr_val:\n",
    "        result.error_field = \"CREDIT_RELEVANT\"\n",
    "        result.error_detail = \"Missing CREDIT_RELEVANT field\"\n",
    "        return result\n",
    "    if cr_val not in vocab.get(\"credit_relevant\", {\"Yes\", \"No\"}):\n",
    "        result.error_field = \"CREDIT_RELEVANT\"\n",
    "        result.error_detail = f\"Invalid value '{cr_val}', expected Yes/No\"\n",
    "        return result\n",
    "    result.credit_relevant = (cr_val == \"Yes\")\n",
    "\n",
    "    if not result.credit_relevant:\n",
    "        reasoning = fields.get(\"REASONING\", \"\").strip()\n",
    "        if not reasoning:\n",
    "            result.error_field = \"REASONING\"\n",
    "            result.error_detail = \"Missing REASONING for non-credit-relevant article\"\n",
    "            return result\n",
    "        result.reasoning = reasoning\n",
    "        if result.has_end_token:\n",
    "            result.parse_ok = True\n",
    "        return result\n",
    "\n",
    "    for field_name, attr, valid_set in [\n",
    "        (\"DIRECTION\", \"direction\", vocab.get(\"direction\", set())),\n",
    "        (\"SIGNAL_TYPE\", \"signal_type\", vocab.get(\"signal_type\", set())),\n",
    "        (\"SECTOR_WIDE\", \"sector_wide\", vocab.get(\"sector_wide\", {\"Yes\", \"No\"})),\n",
    "        (\"CONFIDENCE\", \"confidence\", vocab.get(\"confidence\", set())),\n",
    "    ]:\n",
    "        val = fields.get(field_name, \"\").strip()\n",
    "        if not val:\n",
    "            result.error_field = field_name\n",
    "            result.error_detail = f\"Missing {field_name} field for credit-relevant article\"\n",
    "            return result\n",
    "        if val not in valid_set:\n",
    "            result.error_field = field_name\n",
    "            result.error_detail = f\"Invalid value '{val}', expected one of: {sorted(valid_set)}\"\n",
    "            return result\n",
    "        if field_name == \"SECTOR_WIDE\":\n",
    "            result.sector_wide = (val == \"Yes\")\n",
    "        else:\n",
    "            setattr(result, attr, val)\n",
    "\n",
    "    reasoning = fields.get(\"REASONING\", \"\").strip()\n",
    "    if not reasoning:\n",
    "        result.error_field = \"REASONING\"\n",
    "        result.error_detail = \"Missing REASONING field for credit-relevant article\"\n",
    "        return result\n",
    "    result.reasoning = reasoning\n",
    "    if result.has_end_token:\n",
    "        result.parse_ok = True\n",
    "    return result\n",
    "\n",
    "\n",
    "def classify_failure(raw_output, parsed):\n",
    "    text = raw_output.strip()\n",
    "    refusal_patterns = [\n",
    "        r\"(?i)as an (ai|language model|assistant)\",\n",
    "        r\"(?i)i cannot\", r\"(?i)i'm not able to\",\n",
    "        r\"(?i)i don't have\", r\"(?i)i apologize\",\n",
    "        r\"(?i)sorry,?\\s+(?:but\\s+)?i\",\n",
    "    ]\n",
    "    for pattern in refusal_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return \"refusal\"\n",
    "    known_fields = {\"CREDIT_RELEVANT\", \"DIRECTION\", \"SIGNAL_TYPE\",\n",
    "                    \"SECTOR_WIDE\", \"CONFIDENCE\", \"REASONING\"}\n",
    "    found_fields = set()\n",
    "    for fn in known_fields:\n",
    "        if fn + \":\" in text.upper():\n",
    "            found_fields.add(fn)\n",
    "    if not found_fields:\n",
    "        return \"totally_unstructured\"\n",
    "    if parsed.error_field == \"END\" and parsed.error_detail == \"Missing END token\":\n",
    "        test_parsed = parse_model_output(text + \"\\nEND\")\n",
    "        if test_parsed.parse_ok:\n",
    "            return \"missing_end\"\n",
    "    if \"Invalid value\" in parsed.error_detail:\n",
    "        return \"wrong_vocab\"\n",
    "    if \"Missing\" in parsed.error_detail and parsed.error_field in known_fields:\n",
    "        if len(found_fields) >= 2:\n",
    "            return \"partial_format\"\n",
    "        return \"missing_field\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    non_field_lines = []\n",
    "    for line in lines:\n",
    "        ls = line.strip()\n",
    "        if not ls or ls == \"END\":\n",
    "            continue\n",
    "        is_field = any(ls.upper().startswith(f + \":\") for f in known_fields)\n",
    "        if not is_field:\n",
    "            non_field_lines.append(ls)\n",
    "    if non_field_lines and len(found_fields) >= 3:\n",
    "        return \"extra_content\"\n",
    "    return \"partial_format\"\n",
    "\n",
    "\n",
    "print(\"âœ… Parser and failure classifier defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# Inference helper â€” reused across val, test, and holdout eval\n",
    "# ============================================================\n",
    "\n",
    "def run_inference(model, tokenizer, examples, max_new_tokens=200, desc=\"Inference\"):\n",
    "    \"\"\"Run inference on a list of instruction/input/output dicts.\n",
    "\n",
    "    Returns list of raw output strings (model-generated text).\n",
    "    \"\"\"\n",
    "    raw_outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    for example in tqdm(examples, desc=desc):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True,\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        new_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "        raw_outputs.append(response)\n",
    "\n",
    "    return raw_outputs\n",
    "\n",
    "\n",
    "def evaluate_outputs(raw_outputs, ground_truth, split_name=\"val\"):\n",
    "    \"\"\"Parse outputs, compute metrics, print report.\n",
    "\n",
    "    Returns (parsed_results, summary_dict).\n",
    "    \"\"\"\n",
    "    parsed_results = [parse_model_output(raw) for raw in raw_outputs]\n",
    "    total = len(parsed_results)\n",
    "    parsed_ok = sum(1 for p in parsed_results if p.parse_ok)\n",
    "    failed = total - parsed_ok\n",
    "    parse_rate = parsed_ok / total if total > 0 else 0.0\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"EVALUATION RESULTS â€” {split_name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Total examples:      {total:>6,d}\")\n",
    "    print(f\"  Successfully parsed: {parsed_ok:>6,d}  ({parse_rate:.1%})\")\n",
    "    print(f\"  Parse failures:      {failed:>6,d}  ({1 - parse_rate:.1%})\")\n",
    "    print()\n",
    "\n",
    "    # Failure taxonomy\n",
    "    failure_counts = Counter()\n",
    "    failure_examples = defaultdict(list)\n",
    "    for pred, raw in zip(parsed_results, raw_outputs):\n",
    "        if not pred.parse_ok:\n",
    "            bucket = classify_failure(raw, pred)\n",
    "            failure_counts[bucket] += 1\n",
    "            if len(failure_examples[bucket]) < 2:\n",
    "                failure_examples[bucket].append(raw[:200])\n",
    "\n",
    "    if failure_counts:\n",
    "        print(\"FAILURE MODES:\")\n",
    "        for bucket, count in failure_counts.most_common():\n",
    "            pct = count / failed * 100 if failed > 0 else 0\n",
    "            print(f\"  {bucket:25s}  {count:>5d}  ({pct:5.1f}% of failures)\")\n",
    "            for j, ex in enumerate(failure_examples[bucket], 1):\n",
    "                print(f\"    Ex {j}: {ex[:100]}\")\n",
    "        print()\n",
    "\n",
    "    # Per-field accuracy (only for successfully parsed)\n",
    "    field_metrics = {\n",
    "        \"credit_relevant\": {\"total\": 0, \"correct\": 0},\n",
    "        \"direction\": {\"total\": 0, \"correct\": 0},\n",
    "        \"signal_type\": {\"total\": 0, \"correct\": 0},\n",
    "        \"sector_wide\": {\"total\": 0, \"correct\": 0},\n",
    "        \"confidence\": {\"total\": 0, \"correct\": 0},\n",
    "    }\n",
    "\n",
    "    for pred, gt_dict in zip(parsed_results, ground_truth):\n",
    "        if not pred.parse_ok:\n",
    "            continue\n",
    "        gt = parse_model_output(gt_dict[\"output\"])\n",
    "        if not gt.parse_ok:\n",
    "            continue\n",
    "        field_metrics[\"credit_relevant\"][\"total\"] += 1\n",
    "        if pred.credit_relevant == gt.credit_relevant:\n",
    "            field_metrics[\"credit_relevant\"][\"correct\"] += 1\n",
    "        if pred.credit_relevant and gt.credit_relevant:\n",
    "            for fn, pv, gv in [\n",
    "                (\"direction\", pred.direction, gt.direction),\n",
    "                (\"signal_type\", pred.signal_type, gt.signal_type),\n",
    "                (\"sector_wide\", pred.sector_wide, gt.sector_wide),\n",
    "                (\"confidence\", pred.confidence, gt.confidence),\n",
    "            ]:\n",
    "                field_metrics[fn][\"total\"] += 1\n",
    "                if pv == gv:\n",
    "                    field_metrics[fn][\"correct\"] += 1\n",
    "\n",
    "    if parsed_ok > 0:\n",
    "        print(\"PER-FIELD ACCURACY:\")\n",
    "        print(f\"  {'Field':20s}  {'Total':>6s}  {'Correct':>7s}  {'Accuracy':>8s}\")\n",
    "        print(\"-\" * 50)\n",
    "        for fn, counts in field_metrics.items():\n",
    "            if counts[\"total\"] > 0:\n",
    "                acc = counts[\"correct\"] / counts[\"total\"]\n",
    "                print(f\"  {fn:20s}  {counts['total']:>6d}  {counts['correct']:>7d}  {acc:>8.1%}\")\n",
    "            else:\n",
    "                print(f\"  {fn:20s}  {'N/A':>6s}  {'N/A':>7s}  {'N/A':>8s}\")\n",
    "    print()\n",
    "\n",
    "    summary = {\n",
    "        \"parse_rate\": parse_rate,\n",
    "        \"total\": total,\n",
    "        \"parsed_ok\": parsed_ok,\n",
    "        \"failure_counts\": dict(failure_counts),\n",
    "        \"field_metrics\": field_metrics,\n",
    "    }\n",
    "    return parsed_results, summary\n",
    "\n",
    "\n",
    "# --- Run validation evaluation ---\n",
    "VAL_SAMPLE_SIZE = 500\n",
    "rng = random.Random(42)\n",
    "\n",
    "# Stratified sample from validation set\n",
    "val_cr_yes = [ex for ex in val_raw if ex[\"output\"].startswith(\"CREDIT_RELEVANT: Yes\")]\n",
    "val_cr_no = [ex for ex in val_raw if ex[\"output\"].startswith(\"CREDIT_RELEVANT: No\")]\n",
    "half = VAL_SAMPLE_SIZE // 2\n",
    "val_sample = rng.sample(val_cr_yes, min(half, len(val_cr_yes))) + \\\n",
    "             rng.sample(val_cr_no, min(VAL_SAMPLE_SIZE - min(half, len(val_cr_yes)), len(val_cr_no)))\n",
    "rng.shuffle(val_sample)\n",
    "\n",
    "print(f\"Running inference on {len(val_sample)} validation examples...\")\n",
    "val_raw_outputs = run_inference(model, tokenizer, val_sample, desc=\"Val Inference\")\n",
    "\n",
    "# Save val outputs to Drive\n",
    "val_output_path = os.path.join(DATA_DIR, \"finetuned_val_outputs.jsonl\")\n",
    "with open(val_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex, raw in zip(val_sample, val_raw_outputs):\n",
    "        f.write(json.dumps({\"expected\": ex[\"output\"], \"generated\": raw}, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"âœ… Val outputs saved to {val_output_path}\")\n",
    "\n",
    "val_parsed, val_summary = evaluate_outputs(val_raw_outputs, val_sample, \"validation\")\n",
    "\n",
    "# Compare to baseline\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"â”‚  PARSE RATE: {val_summary['parse_rate']:.1%} (fine-tuned) vs 0.0% (base model)  â”‚\")\n",
    "if val_summary['parse_rate'] >= 0.80:\n",
    "    print(f\"â”‚  âœ… TARGET MET: >80% parse rate                          â”‚\")\n",
    "else:\n",
    "    print(f\"â”‚  âš ï¸  Below 80% target â€” check failure modes above        â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Evaluation â€” Test Set (The Honest Number)\n",
    "\n",
    "ğŸ“ **Why val and test are different:**\n",
    "- **Validation (Cell 8):** Model tuning feedback. We used val loss to select the best\n",
    "  checkpoint during training. So val results are slightly optimistic â€” the model was\n",
    "  indirectly \"tuned\" to this data.\n",
    "- **Test (this cell):** The honest number. Forward-looking period (2023-07 to 2024-12)\n",
    "  that was NEVER used during training in any way. This simulates production deployment:\n",
    "  \"how well does the model perform on future articles?\"\n",
    "\n",
    "If test parse rate is much worse than val, the model overfit to training-era patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_raw = load_jsonl(os.path.join(DATA_DIR, \"test.jsonl\"))\n",
    "\n",
    "# Sample 500 from test set (stratified)\n",
    "TEST_SAMPLE_SIZE = 500\n",
    "test_cr_yes = [ex for ex in test_raw if ex[\"output\"].startswith(\"CREDIT_RELEVANT: Yes\")]\n",
    "test_cr_no = [ex for ex in test_raw if ex[\"output\"].startswith(\"CREDIT_RELEVANT: No\")]\n",
    "half = TEST_SAMPLE_SIZE // 2\n",
    "test_sample = rng.sample(test_cr_yes, min(half, len(test_cr_yes))) + \\\n",
    "              rng.sample(test_cr_no, min(TEST_SAMPLE_SIZE - min(half, len(test_cr_yes)), len(test_cr_no)))\n",
    "rng.shuffle(test_sample)\n",
    "\n",
    "print(f\"Running inference on {len(test_sample)} test examples...\")\n",
    "test_raw_outputs = run_inference(model, tokenizer, test_sample, desc=\"Test Inference\")\n",
    "\n",
    "# Save test outputs\n",
    "test_output_path = os.path.join(DATA_DIR, \"finetuned_test_outputs.jsonl\")\n",
    "with open(test_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex, raw in zip(test_sample, test_raw_outputs):\n",
    "        f.write(json.dumps({\"expected\": ex[\"output\"], \"generated\": raw}, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"âœ… Test outputs saved to {test_output_path}\")\n",
    "\n",
    "test_parsed, test_summary = evaluate_outputs(test_raw_outputs, test_sample, \"test\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"â”‚  VAL parse rate:  {val_summary['parse_rate']:>6.1%}                          â”‚\")\n",
    "print(f\"â”‚  TEST parse rate: {test_summary['parse_rate']:>6.1%}  â† the honest number     â”‚\")\n",
    "print(f\"â”‚  BASE parse rate:  0.0%  (Phase 2.1 baseline)        â”‚\")\n",
    "gap = abs(val_summary['parse_rate'] - test_summary['parse_rate'])\n",
    "if gap > 0.10:\n",
    "    print(f\"â”‚  âš ï¸  Valâ†’Test gap: {gap:.1%} â€” possible overfitting      â”‚\")\n",
    "else:\n",
    "    print(f\"â”‚  âœ… Valâ†’Test gap: {gap:.1%} â€” stable generalization      â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Evaluation â€” Entity Holdout (The Money Shot)\n",
    "\n",
    "ğŸ“ **Why this is the most important eval:**\n",
    "\n",
    "The entity holdout tests whether the model learned **text patterns** or just **entity names**.\n",
    "\n",
    "These 3 entities were completely removed from training:\n",
    "- **DHFL** (1,243 articles, 91% deterioration): Did the model learn to detect distress\n",
    "  signals from NPAs, defaults, and regulatory actions â€” or did it just memorize \"DHFL = bad\"?\n",
    "- **Reliance Capital** (688 articles, 80% deterioration): Different crisis arc (insurance\n",
    "  sector, not housing). Can the model generalize across crisis types?\n",
    "- **Cholamandalam** (1,372 articles, 12% deterioration): The false-positive control.\n",
    "  A healthy NBFC with mostly positive/neutral news. If the model says \"Deterioration\"\n",
    "  for Cholamandalam articles, it's just guessing \"all NBFCs are risky.\"\n",
    "\n",
    "The ideal result: high recall on DHFL/RelCap deterioration + low false positive rate on Cholamandalam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full entity holdout set\n",
    "holdout_raw = load_jsonl(os.path.join(DATA_DIR, \"entity_holdout.jsonl\"))\n",
    "print(f\"Entity holdout: {len(holdout_raw):,d} examples\")\n",
    "\n",
    "# Extract entity names\n",
    "def extract_entity(input_text):\n",
    "    for line in input_text.split(\"\\n\"):\n",
    "        if line.startswith(\"Entity:\"):\n",
    "            return line[len(\"Entity:\"):].strip()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "holdout_entities = [extract_entity(ex[\"input\"]) for ex in holdout_raw]\n",
    "entity_counts = Counter(holdout_entities)\n",
    "print(f\"Entities: {dict(entity_counts)}\")\n",
    "\n",
    "# Run inference on ALL holdout examples\n",
    "# This takes ~40-60 min on T4. It's worth the wait â€” this is the definitive eval.\n",
    "print(f\"\\nRunning inference on {len(holdout_raw):,d} holdout examples...\")\n",
    "print(f\"Estimated time: ~{len(holdout_raw) * 1.5 / 60:.0f} min on T4\")\n",
    "holdout_raw_outputs = run_inference(model, tokenizer, holdout_raw, desc=\"Holdout Inference\")\n",
    "\n",
    "# Save holdout outputs\n",
    "holdout_output_path = os.path.join(DATA_DIR, \"finetuned_holdout_outputs.jsonl\")\n",
    "with open(holdout_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex, raw, entity in zip(holdout_raw, holdout_raw_outputs, holdout_entities):\n",
    "        f.write(json.dumps({\n",
    "            \"entity\": entity,\n",
    "            \"expected\": ex[\"output\"],\n",
    "            \"generated\": raw,\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"âœ… Holdout outputs saved to {holdout_output_path}\")\n",
    "\n",
    "# Overall holdout parse eval\n",
    "holdout_parsed, holdout_summary = evaluate_outputs(\n",
    "    holdout_raw_outputs, holdout_raw, \"entity holdout (aggregate)\"\n",
    ")\n",
    "\n",
    "# --- Per-entity evaluation ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PER-ENTITY HOLDOUT EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ENTITY_INFO = {\n",
    "    \"DHFL\": \"Crisis detection (91% det.) â€” did it learn distress patterns?\",\n",
    "    \"Reliance Capital\": \"Generalization (80% det.) â€” different crisis type?\",\n",
    "    \"Cholamandalam\": \"False positive control (12% det.) â€” stable NBFC\",\n",
    "}\n",
    "\n",
    "for entity in [\"DHFL\", \"Reliance Capital\", \"Cholamandalam\"]:\n",
    "    indices = [i for i, e in enumerate(holdout_entities) if e == entity]\n",
    "    if not indices:\n",
    "        print(f\"\\n  {entity}: No examples found\")\n",
    "        continue\n",
    "\n",
    "    e_raw_outs = [holdout_raw_outputs[i] for i in indices]\n",
    "    e_ground_truth = [holdout_raw[i] for i in indices]\n",
    "    e_parsed = [holdout_parsed[i] for i in indices]\n",
    "\n",
    "    total = len(indices)\n",
    "    parsed_ok = sum(1 for p in e_parsed if p.parse_ok)\n",
    "    pr = parsed_ok / total if total > 0 else 0.0\n",
    "\n",
    "    # Per-field accuracy\n",
    "    cr_correct = 0\n",
    "    dir_correct = dir_total = 0\n",
    "    st_correct = st_total = 0\n",
    "    det_tp = det_fp = det_fn = 0\n",
    "    imp_tp = imp_fp = imp_fn = 0\n",
    "\n",
    "    for pred, gt_dict in zip(e_parsed, e_ground_truth):\n",
    "        if not pred.parse_ok:\n",
    "            continue\n",
    "        gt = parse_model_output(gt_dict[\"output\"])\n",
    "        if not gt.parse_ok:\n",
    "            continue\n",
    "        if pred.credit_relevant == gt.credit_relevant:\n",
    "            cr_correct += 1\n",
    "        if pred.credit_relevant and gt.credit_relevant:\n",
    "            dir_total += 1\n",
    "            if pred.direction == gt.direction:\n",
    "                dir_correct += 1\n",
    "            st_total += 1\n",
    "            if pred.signal_type == gt.signal_type:\n",
    "                st_correct += 1\n",
    "\n",
    "        pred_det = pred.credit_relevant and pred.direction == \"Deterioration\"\n",
    "        gt_det = gt.credit_relevant and gt.direction == \"Deterioration\"\n",
    "        pred_imp = pred.credit_relevant and pred.direction == \"Improvement\"\n",
    "        gt_imp = gt.credit_relevant and gt.direction == \"Improvement\"\n",
    "\n",
    "        if pred_det and gt_det: det_tp += 1\n",
    "        elif pred_det and not gt_det: det_fp += 1\n",
    "        elif not pred_det and gt_det: det_fn += 1\n",
    "        if pred_imp and gt_imp: imp_tp += 1\n",
    "        elif pred_imp and not gt_imp: imp_fp += 1\n",
    "        elif not pred_imp and gt_imp: imp_fn += 1\n",
    "\n",
    "    info = ENTITY_INFO.get(entity, \"\")\n",
    "    print(f\"\\n  {entity} ({total} articles) â€” {info}\")\n",
    "    print(f\"  {'â”€' * 60}\")\n",
    "    print(f\"    Parse rate:         {pr:>6.1%}  ({parsed_ok}/{total})\")\n",
    "    if parsed_ok > 0:\n",
    "        print(f\"    CR accuracy:        {cr_correct / parsed_ok:>6.1%}\")\n",
    "        print(f\"    Direction accuracy:  {dir_correct / dir_total:>6.1%}\" if dir_total > 0 else \"    Direction accuracy:    N/A\")\n",
    "        print(f\"    Signal type acc:    {st_correct / st_total:>6.1%}\" if st_total > 0 else \"    Signal type acc:      N/A\")\n",
    "        det_prec = det_tp / (det_tp + det_fp) if (det_tp + det_fp) > 0 else 0.0\n",
    "        det_rec = det_tp / (det_tp + det_fn) if (det_tp + det_fn) > 0 else 0.0\n",
    "        imp_prec = imp_tp / (imp_tp + imp_fp) if (imp_tp + imp_fp) > 0 else 0.0\n",
    "        imp_rec = imp_tp / (imp_tp + imp_fn) if (imp_tp + imp_fn) > 0 else 0.0\n",
    "        print(f\"    Det. precision:     {det_prec:>6.1%}  (of predicted det., how many correct)\")\n",
    "        print(f\"    Det. recall:        {det_rec:>6.1%}  (of actual det., how many caught)\")\n",
    "        print(f\"    Imp. precision:     {imp_prec:>6.1%}\")\n",
    "        print(f\"    Imp. recall:        {imp_rec:>6.1%}\")\n",
    "\n",
    "    # Failure modes for this entity\n",
    "    e_failures = Counter()\n",
    "    for pred, raw in zip(e_parsed, e_raw_outs):\n",
    "        if not pred.parse_ok:\n",
    "            e_failures[classify_failure(raw, pred)] += 1\n",
    "    if e_failures:\n",
    "        print(f\"    Failure modes: {dict(e_failures)}\")\n",
    "\n",
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2.2 â€” TRAINING COMPLETE â€” FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\")\n",
    "print(f\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"  â”‚  Model:   Qwen 2.5-7B-Instruct + LoRA (r=16, 5 modules)      â”‚\")\n",
    "print(f\"  â”‚  Train:   {len(train_raw):>6,d} examples, {training_args.num_train_epochs} epochs                        â”‚\")\n",
    "print(f\"  â”‚                                                                â”‚\")\n",
    "print(f\"  â”‚  Parse rates:                                                  â”‚\")\n",
    "print(f\"  â”‚    Base model (Phase 2.1):   0.0%                              â”‚\")\n",
    "print(f\"  â”‚    Validation (post-train): {val_summary['parse_rate']:>5.1%}                              â”‚\")\n",
    "print(f\"  â”‚    Test (honest number):    {test_summary['parse_rate']:>5.1%}                              â”‚\")\n",
    "print(f\"  â”‚    Entity holdout:          {holdout_summary['parse_rate']:>5.1%}                              â”‚\")\n",
    "print(f\"  â”‚                                                                â”‚\")\n",
    "print(f\"  â”‚  Adapters saved to: {MODEL_DIR}  â”‚\")\n",
    "print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "print(f\"  â­ï¸  NEXT STEPS:\")\n",
    "print(f\"  1. Review per-entity holdout results above\")\n",
    "print(f\"  2. If parse rate >80% â†’ model is working, proceed to Phase 2.3 (RLMF)\")\n",
    "print(f\"  3. If field accuracy is low â†’ may need more training data or adjusted hyperparams\")\n",
    "print(f\"  4. Download adapters from Drive for git commit\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nâœ… GPU memory freed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}