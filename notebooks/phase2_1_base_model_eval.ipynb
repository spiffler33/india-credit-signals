{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2.1 â€” Base Model Evaluation (Qwen 2.5-7B, No Fine-Tuning)\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads Qwen 2.5-7B-Instruct in 4-bit quantization (saves ~75% VRAM)\n",
    "2. Feeds 1,000 stratified training examples (500 credit-relevant, 500 not)\n",
    "3. Parses every output with our strict parser\n",
    "4. Reports: parse success rate, per-field accuracy, failure mode taxonomy\n",
    "5. Prints a GO/NO-GO decision for proceeding to LoRA training\n",
    "\n",
    "**Why this matters before training:**\n",
    "If the base model can't even produce our output format >20% of the time, fine-tuning\n",
    "will waste $50-100 fighting format compliance instead of learning credit signals.\n",
    "This test establishes a performance floor â€” any fine-tuned model must beat these numbers.\n",
    "\n",
    "**Expected outcome:** Base model will probably get <80% parse rate (it's never seen our\n",
    "format). But even 40-60% means the format is learnable. <20% means we should simplify.\n",
    "\n",
    "**Time estimate:** ~30-45 min on T4, ~15-20 min on A100 (sequential inference).\n",
    "\n",
    "**Data location:** `drive/MyDrive/india-credit-signals/data/processed/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup â€” GPU, Packages, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Step 1a: Verify GPU allocation\n",
    "# Colab assigns you a GPU when you select Runtime â†’ Change runtime type â†’ GPU.\n",
    "# T4 (15GB VRAM) is the free tier default. A100 (40GB) is Colab Pro.\n",
    "# With 4-bit quantization, Qwen 7B needs ~5GB VRAM â€” T4 is fine.\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    print(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU! Go to Runtime â†’ Change runtime type â†’ select GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Step 1b: Install dependencies\n",
    "# We use pip here (not uv) because Colab's pre-built environment conflicts with uv venvs.\n",
    "# bitsandbytes: handles 4-bit quantization (QLoRA-style weight compression)\n",
    "# accelerate: HuggingFace's multi-GPU/mixed-precision toolkit (needed by transformers)\n",
    "# tqdm: progress bars for the inference loop\n",
    "!pip install -q transformers>=4.48.0 accelerate>=1.0.0 bitsandbytes>=0.43.0 tqdm\n",
    "\n",
    "import transformers, accelerate, bitsandbytes\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"accelerate: {accelerate.__version__}\")\n",
    "print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Step 1c: Mount Google Drive and locate data files\n",
    "# Google Drive is the bridge between your local Mac (where Claude Code runs)\n",
    "# and Colab (where GPU inference runs). Push data to Drive, pull results back.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DATA_DIR = \"/content/drive/MyDrive/india-credit-signals/data/processed\"\n",
    "assert os.path.isdir(DATA_DIR), f\"Data directory not found: {DATA_DIR}\\nUpload your data to Google Drive first.\"\n",
    "\n",
    "# Verify key files exist\n",
    "for fname in [\"train.jsonl\", \"val.jsonl\", \"test.jsonl\", \"entity_holdout.jsonl\"]:\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        # Count lines\n",
    "        with open(fpath) as f:\n",
    "            n = sum(1 for _ in f)\n",
    "        print(f\"  âœ… {fname}: {n:,d} examples\")\n",
    "    else:\n",
    "        print(f\"  âŒ {fname}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nâœ… Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Step 1d: Load Qwen 2.5-7B-Instruct in 4-bit quantization\n",
    "#\n",
    "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "# â”‚ ğŸ“ CONCEPT: 4-bit Quantization (QLoRA-style)            â”‚\n",
    "# â”‚                                                          â”‚\n",
    "# â”‚ A 7B model normally needs ~14GB in fp16 (2 bytes/param). â”‚\n",
    "# â”‚ 4-bit quantization compresses each weight to 4 bits,     â”‚\n",
    "# â”‚ reducing memory to ~3.5-5GB.                             â”‚\n",
    "# â”‚                                                          â”‚\n",
    "# â”‚ NF4 (Normal Float 4) is a special 4-bit format designed  â”‚\n",
    "# â”‚ for neural network weights. It maps the 16 possible 4-   â”‚\n",
    "# â”‚ bit values to the most common weight values, minimizing   â”‚\n",
    "# â”‚ quantization error.                                      â”‚\n",
    "# â”‚                                                          â”‚\n",
    "# â”‚ \"double_quant\" quantizes the quantization constants      â”‚\n",
    "# â”‚ themselves â€” saves another ~0.4GB for free.              â”‚\n",
    "# â”‚                                                          â”‚\n",
    "# â”‚ Quality loss: ~1-2% accuracy drop vs fp16. Acceptable    â”‚\n",
    "# â”‚ for testing; we'll use fp16 for final training.          â”‚\n",
    "# â”‚                                                          â”‚\n",
    "# â”‚ Paper: https://arxiv.org/abs/2305.14314                  â”‚\n",
    "# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# ğŸ“ BitsAndBytesConfig controls the quantization strategy.\n",
    "# load_in_4bit=True: compress all weights to 4 bits\n",
    "# bnb_4bit_quant_type=\"nf4\": use NormalFloat4 (better than uniform 4-bit)\n",
    "# bnb_4bit_compute_dtype=float16: do actual math in fp16 (not 4-bit)\n",
    "# bnb_4bit_use_double_quant=True: quantize the scale factors too (saves memory)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} in 4-bit...\")\n",
    "t0 = time.time()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # ğŸ“ Places model layers across available GPU(s) automatically\n",
    ")\n",
    "\n",
    "load_time = time.time() - t0\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "gpu_mem_used = torch.cuda.memory_allocated() / 1e9\n",
    "\n",
    "print(f\"âœ… Loaded in {load_time:.1f}s\")\n",
    "print(f\"   Parameters: {param_count / 1e9:.2f}B\")\n",
    "print(f\"   GPU memory used: {gpu_mem_used:.1f} GB\")\n",
    "print(f\"   Quantization: 4-bit NF4 with double quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load & Sample Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def load_jsonl(path):\n",
    "    \"\"\"Load JSONL file into list of dicts.\"\"\"\n",
    "    records = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "# Load training data\n",
    "train_data = load_jsonl(os.path.join(DATA_DIR, \"train.jsonl\"))\n",
    "print(f\"Loaded {len(train_data):,d} training examples\")\n",
    "\n",
    "# ğŸ“ Stratified sampling: 50% credit-relevant, 50% not.\n",
    "# This gives equal coverage of full-form output (6 fields + END) and\n",
    "# short-form output (2 fields + END). Both formats need to parse correctly.\n",
    "# If we sampled randomly, we'd get ~54% credit-relevant (natural distribution),\n",
    "# which is close to 50/50 anyway â€” but explicit stratification is cleaner.\n",
    "\n",
    "SAMPLE_SIZE = 1000\n",
    "SEED = 42\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "cr_yes = [ex for ex in train_data if ex.get(\"output\", \"\").startswith(\"CREDIT_RELEVANT: Yes\")]\n",
    "cr_no = [ex for ex in train_data if ex.get(\"output\", \"\").startswith(\"CREDIT_RELEVANT: No\")]\n",
    "\n",
    "half = SAMPLE_SIZE // 2\n",
    "sample_yes = rng.sample(cr_yes, min(half, len(cr_yes)))\n",
    "sample_no = rng.sample(cr_no, min(SAMPLE_SIZE - len(sample_yes), len(cr_no)))\n",
    "sample = sample_yes + sample_no\n",
    "rng.shuffle(sample)\n",
    "\n",
    "print(f\"\\nSampled {len(sample)} examples:\")\n",
    "print(f\"  Credit-relevant (full-form):  {len(sample_yes)}\")\n",
    "print(f\"  Not relevant (short-form):    {len(sample_no)}\")\n",
    "\n",
    "# Show one example so you know what the model will see\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE INPUT (first 500 chars):\")\n",
    "print(\"=\" * 70)\n",
    "print(sample[0][\"input\"][:500])\n",
    "print(\"\\nEXPECTED OUTPUT:\")\n",
    "print(sample[0][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Inference â€” Run Base Model on 1,000 Examples\n",
    "\n",
    "ğŸ“ **What's happening here:** We send each example's `instruction + input` to Qwen 2.5-7B\n",
    "and collect whatever it generates. The model has NEVER seen our output format before.\n",
    "It will try its best based on general instruction-following ability, but it hasn't been\n",
    "trained on our specific structured text format. That's the whole point â€” measuring the gap\n",
    "between \"general intelligence\" and \"trained for our task.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ğŸ“ WHY sequential and not batched: 4-bit quantized models on Colab are\n",
    "# memory-constrained. Batching requires padding all inputs to max length,\n",
    "# which wastes VRAM on our variable-length articles. Sequential with KV-cache\n",
    "# is simpler and more reliable. At ~1-2s per example, 1,000 takes ~20-30 min.\n",
    "\n",
    "# ğŸ“ max_new_tokens=200: Our full-form output is ~7 lines (~100 tokens).\n",
    "# Short-form is ~3 lines (~30 tokens). 200 gives generous headroom for\n",
    "# verbose base models without wasting too much time on runaway generation.\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "raw_outputs = []\n",
    "inference_times = []\n",
    "\n",
    "print(f\"Running inference on {len(sample)} examples...\")\n",
    "print(f\"Estimated time: ~{len(sample) * 2 / 60:.0f} min on T4, ~{len(sample) * 1 / 60:.0f} min on A100\")\n",
    "print()\n",
    "\n",
    "for i, example in enumerate(tqdm(sample, desc=\"Inference\")):\n",
    "    # Build the prompt using Qwen's chat template\n",
    "    # ğŸ“ Qwen 2.5 Instruct uses a chat format with <|im_start|> tokens.\n",
    "    # apply_chat_template handles this automatically. We put the instruction\n",
    "    # in the system message and the article in the user message.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  # ğŸ“ Adds the assistant turn start token\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,       # ğŸ“ Greedy decoding: deterministic, reproducible\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            # ğŸ“ Stop at END token or common stop sequences\n",
    "            # The model may not know to stop at \"END\", so we also set eos\n",
    "        )\n",
    "    gen_time = time.time() - t0\n",
    "    inference_times.append(gen_time)\n",
    "\n",
    "    # Extract only the newly generated tokens (not the prompt)\n",
    "    new_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    raw_outputs.append(response)\n",
    "\n",
    "    # Show progress every 100 examples\n",
    "    if (i + 1) % 100 == 0:\n",
    "        avg_time = sum(inference_times[-100:]) / len(inference_times[-100:])\n",
    "        remaining = (len(sample) - i - 1) * avg_time / 60\n",
    "        tqdm.write(f\"  [{i+1}/{len(sample)}] avg {avg_time:.1f}s/example, ~{remaining:.0f} min remaining\")\n",
    "\n",
    "total_time = sum(inference_times)\n",
    "print(f\"\\nâœ… Inference complete: {total_time / 60:.1f} min total, {total_time / len(sample):.1f}s avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw outputs to Drive (crash-safe â€” don't lose 30 min of inference)\n",
    "output_path = os.path.join(DATA_DIR, \"base_model_outputs.jsonl\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example, raw_out in zip(sample, raw_outputs):\n",
    "        record = {\n",
    "            \"input\": example[\"input\"][:200],  # Truncated for storage\n",
    "            \"expected_output\": example[\"output\"],\n",
    "            \"model_output\": raw_out,\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Saved {len(raw_outputs)} outputs to {output_path}\")\n",
    "\n",
    "# Show a few raw outputs so you can see what the model actually produces\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAMPLE RAW OUTPUTS (first 5):\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(5, len(raw_outputs))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Expected: {sample[i]['output'][:100]}...\")\n",
    "    print(f\"Got:      {raw_outputs[i][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Parse Evaluation â€” Strict Parser + Failure Taxonomy\n",
    "\n",
    "ğŸ“ **Source of truth for this parser:** `src/data/parse_training_output.py`  \n",
    "The code below is copied from that file to make this notebook self-contained on Colab.  \n",
    "If you change the parser, update it in the source file first, then copy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRICT OUTPUT PARSER\n",
    "# ğŸ“ SOURCE OF TRUTH: src/data/parse_training_output.py\n",
    "# This is a copy for Colab self-containment. If you modify the\n",
    "# parser, update the source file first, then copy changes here.\n",
    "# The canonical evaluation module is src/training/evaluate.py.\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass, field as dataclass_field\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Vocabulary â€” must match configs/training_config.yaml\n",
    "VOCAB = {\n",
    "    \"credit_relevant\": {\"Yes\", \"No\"},\n",
    "    \"direction\": {\"Deterioration\", \"Improvement\", \"Neutral\"},\n",
    "    \"signal_type\": {\n",
    "        \"liquidity\", \"asset_quality\", \"regulatory\", \"contagion\",\n",
    "        \"governance\", \"funding\", \"operational\", \"other\",\n",
    "    },\n",
    "    \"sector_wide\": {\"Yes\", \"No\"},\n",
    "    \"confidence\": {\"Low\", \"Medium\", \"High\"},\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParsedOutput:\n",
    "    \"\"\"Result of parsing a model's structured text output.\"\"\"\n",
    "    credit_relevant: bool = False\n",
    "    direction: str = \"\"\n",
    "    signal_type: str = \"\"\n",
    "    sector_wide: bool = False\n",
    "    confidence: str = \"\"\n",
    "    reasoning: str = \"\"\n",
    "    has_end_token: bool = False\n",
    "    parse_ok: bool = False\n",
    "    error_field: str = \"\"\n",
    "    error_detail: str = \"\"\n",
    "\n",
    "\n",
    "def parse_model_output(text, vocab=None):\n",
    "    \"\"\"Parse structured text output into a ParsedOutput.\n",
    "\n",
    "    Handles both full-form (credit-relevant) and short-form (not credit-relevant).\n",
    "    \"\"\"\n",
    "    if vocab is None:\n",
    "        vocab = VOCAB\n",
    "\n",
    "    result = ParsedOutput()\n",
    "    text = text.strip()\n",
    "\n",
    "    # Check for END token\n",
    "    if text.endswith(\"END\"):\n",
    "        result.has_end_token = True\n",
    "        text = text[:-3].strip()\n",
    "    else:\n",
    "        result.error_field = \"END\"\n",
    "        result.error_detail = \"Missing END token\"\n",
    "\n",
    "    # Parse line by line into field â†’ value dict\n",
    "    fields = {}\n",
    "    current_key = None\n",
    "    current_val = \"\"\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        matched = False\n",
    "        for field_name in (\"CREDIT_RELEVANT\", \"DIRECTION\", \"SIGNAL_TYPE\",\n",
    "                           \"SECTOR_WIDE\", \"CONFIDENCE\", \"REASONING\"):\n",
    "            prefix = field_name + \":\"\n",
    "            if line.upper().startswith(prefix.upper()):\n",
    "                if current_key is not None:\n",
    "                    fields[current_key] = current_val.strip()\n",
    "                current_key = field_name\n",
    "                current_val = line[len(prefix):].strip()\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched and current_key is not None:\n",
    "            current_val += \" \" + line\n",
    "\n",
    "    if current_key is not None:\n",
    "        fields[current_key] = current_val.strip()\n",
    "\n",
    "    # --- Validate CREDIT_RELEVANT ---\n",
    "    cr_val = fields.get(\"CREDIT_RELEVANT\", \"\").strip()\n",
    "    if not cr_val:\n",
    "        result.error_field = \"CREDIT_RELEVANT\"\n",
    "        result.error_detail = \"Missing CREDIT_RELEVANT field\"\n",
    "        return result\n",
    "\n",
    "    if cr_val not in vocab.get(\"credit_relevant\", {\"Yes\", \"No\"}):\n",
    "        result.error_field = \"CREDIT_RELEVANT\"\n",
    "        result.error_detail = f\"Invalid value '{cr_val}', expected Yes/No\"\n",
    "        return result\n",
    "\n",
    "    result.credit_relevant = (cr_val == \"Yes\")\n",
    "\n",
    "    # --- Short-form path ---\n",
    "    if not result.credit_relevant:\n",
    "        reasoning = fields.get(\"REASONING\", \"\").strip()\n",
    "        if not reasoning:\n",
    "            result.error_field = \"REASONING\"\n",
    "            result.error_detail = \"Missing REASONING for non-credit-relevant article\"\n",
    "            return result\n",
    "        result.reasoning = reasoning\n",
    "        if result.has_end_token:\n",
    "            result.parse_ok = True\n",
    "        return result\n",
    "\n",
    "    # --- Full-form path ---\n",
    "    dir_val = fields.get(\"DIRECTION\", \"\").strip()\n",
    "    if not dir_val:\n",
    "        result.error_field = \"DIRECTION\"\n",
    "        result.error_detail = \"Missing DIRECTION field for credit-relevant article\"\n",
    "        return result\n",
    "    if dir_val not in vocab.get(\"direction\", set()):\n",
    "        result.error_field = \"DIRECTION\"\n",
    "        result.error_detail = f\"Invalid value '{dir_val}', expected one of: {sorted(vocab.get('direction', set()))}\"\n",
    "        return result\n",
    "    result.direction = dir_val\n",
    "\n",
    "    st_val = fields.get(\"SIGNAL_TYPE\", \"\").strip()\n",
    "    if not st_val:\n",
    "        result.error_field = \"SIGNAL_TYPE\"\n",
    "        result.error_detail = \"Missing SIGNAL_TYPE field for credit-relevant article\"\n",
    "        return result\n",
    "    if st_val not in vocab.get(\"signal_type\", set()):\n",
    "        result.error_field = \"SIGNAL_TYPE\"\n",
    "        result.error_detail = f\"Invalid value '{st_val}', expected one of: {sorted(vocab.get('signal_type', set()))}\"\n",
    "        return result\n",
    "    result.signal_type = st_val\n",
    "\n",
    "    sw_val = fields.get(\"SECTOR_WIDE\", \"\").strip()\n",
    "    if not sw_val:\n",
    "        result.error_field = \"SECTOR_WIDE\"\n",
    "        result.error_detail = \"Missing SECTOR_WIDE field for credit-relevant article\"\n",
    "        return result\n",
    "    if sw_val not in vocab.get(\"sector_wide\", {\"Yes\", \"No\"}):\n",
    "        result.error_field = \"SECTOR_WIDE\"\n",
    "        result.error_detail = f\"Invalid value '{sw_val}', expected Yes/No\"\n",
    "        return result\n",
    "    result.sector_wide = (sw_val == \"Yes\")\n",
    "\n",
    "    conf_val = fields.get(\"CONFIDENCE\", \"\").strip()\n",
    "    if not conf_val:\n",
    "        result.error_field = \"CONFIDENCE\"\n",
    "        result.error_detail = \"Missing CONFIDENCE field for credit-relevant article\"\n",
    "        return result\n",
    "    if conf_val not in vocab.get(\"confidence\", set()):\n",
    "        result.error_field = \"CONFIDENCE\"\n",
    "        result.error_detail = f\"Invalid value '{conf_val}', expected one of: {sorted(vocab.get('confidence', set()))}\"\n",
    "        return result\n",
    "    result.confidence = conf_val\n",
    "\n",
    "    reasoning = fields.get(\"REASONING\", \"\").strip()\n",
    "    if not reasoning:\n",
    "        result.error_field = \"REASONING\"\n",
    "        result.error_detail = \"Missing REASONING field for credit-relevant article\"\n",
    "        return result\n",
    "    result.reasoning = reasoning\n",
    "\n",
    "    if result.has_end_token:\n",
    "        result.parse_ok = True\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def classify_failure(raw_output, parsed):\n",
    "    \"\"\"Classify a parse failure into a taxonomy bucket.\"\"\"\n",
    "    text = raw_output.strip()\n",
    "\n",
    "    # Check for AI refusals\n",
    "    refusal_patterns = [\n",
    "        r\"(?i)as an (ai|language model|assistant)\",\n",
    "        r\"(?i)i cannot\", r\"(?i)i'm not able to\",\n",
    "        r\"(?i)i don't have\", r\"(?i)i apologize\",\n",
    "        r\"(?i)sorry,?\\s+(?:but\\s+)?i\",\n",
    "    ]\n",
    "    for pattern in refusal_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return \"refusal\"\n",
    "\n",
    "    # Check if ANY known field names appear\n",
    "    known_fields = {\"CREDIT_RELEVANT\", \"DIRECTION\", \"SIGNAL_TYPE\",\n",
    "                    \"SECTOR_WIDE\", \"CONFIDENCE\", \"REASONING\"}\n",
    "    found_fields = set()\n",
    "    for field_name in known_fields:\n",
    "        if field_name + \":\" in text.upper():\n",
    "            found_fields.add(field_name)\n",
    "\n",
    "    if not found_fields:\n",
    "        return \"totally_unstructured\"\n",
    "\n",
    "    # Missing END only\n",
    "    if parsed.error_field == \"END\" and parsed.error_detail == \"Missing END token\":\n",
    "        test_parsed = parse_model_output(text + \"\\nEND\")\n",
    "        if test_parsed.parse_ok:\n",
    "            return \"missing_end\"\n",
    "\n",
    "    # Wrong vocab\n",
    "    if \"Invalid value\" in parsed.error_detail:\n",
    "        return \"wrong_vocab\"\n",
    "\n",
    "    # Missing field\n",
    "    if \"Missing\" in parsed.error_detail and parsed.error_field in known_fields:\n",
    "        if len(found_fields) >= 2:\n",
    "            return \"partial_format\"\n",
    "        return \"missing_field\"\n",
    "\n",
    "    # Extra content\n",
    "    lines = text.split(\"\\n\")\n",
    "    non_field_lines = []\n",
    "    for line in lines:\n",
    "        ls = line.strip()\n",
    "        if not ls or ls == \"END\":\n",
    "            continue\n",
    "        is_field = any(ls.upper().startswith(f + \":\") for f in known_fields)\n",
    "        if not is_field:\n",
    "            non_field_lines.append(ls)\n",
    "    if non_field_lines and len(found_fields) >= 3:\n",
    "        return \"extra_content\"\n",
    "\n",
    "    return \"partial_format\"\n",
    "\n",
    "\n",
    "print(\"âœ… Parser and failure classifier defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the strict parser on every output\n",
    "parsed_results = [parse_model_output(raw) for raw in raw_outputs]\n",
    "\n",
    "total = len(parsed_results)\n",
    "parsed_ok = sum(1 for p in parsed_results if p.parse_ok)\n",
    "failed = total - parsed_ok\n",
    "parse_rate = parsed_ok / total if total > 0 else 0.0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PARSE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Total examples:      {total:>6,d}\")\n",
    "print(f\"  Successfully parsed: {parsed_ok:>6,d}  ({parse_rate:.1%})\")\n",
    "print(f\"  Parse failures:      {failed:>6,d}  ({1 - parse_rate:.1%})\")\n",
    "print()\n",
    "\n",
    "# Failure mode taxonomy\n",
    "failure_counts = Counter()\n",
    "failure_examples = defaultdict(list)\n",
    "\n",
    "for pred, raw in zip(parsed_results, raw_outputs):\n",
    "    if not pred.parse_ok:\n",
    "        bucket = classify_failure(raw, pred)\n",
    "        failure_counts[bucket] += 1\n",
    "        if len(failure_examples[bucket]) < 3:\n",
    "            truncated = raw[:300] + \"...\" if len(raw) > 300 else raw\n",
    "            failure_examples[bucket].append(truncated)\n",
    "\n",
    "if failure_counts:\n",
    "    print(\"FAILURE MODE TAXONOMY:\")\n",
    "    print(\"-\" * 70)\n",
    "    for bucket, count in failure_counts.most_common():\n",
    "        pct_of_failures = count / failed * 100 if failed > 0 else 0\n",
    "        pct_of_total = count / total * 100\n",
    "        print(f\"\\n  {bucket:25s}  {count:>5d}  ({pct_of_failures:5.1f}% of failures, {pct_of_total:4.1f}% of total)\")\n",
    "        for j, example in enumerate(failure_examples[bucket], 1):\n",
    "            print(f\"    Example {j}:\")\n",
    "            # Indent each line of the example for readability\n",
    "            for line in example.split(\"\\n\")[:5]:  # Max 5 lines per example\n",
    "                print(f\"      {line}\")\n",
    "else:\n",
    "    print(\"No parse failures! (Unexpected for a base model â€” double-check the parser.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-field accuracy (only for successfully parsed outputs)\n",
    "# ğŸ“ Even if the model gets the FORMAT right, it might get the CONTENT wrong.\n",
    "# A model that always says \"CREDIT_RELEVANT: Yes\" with correct format would\n",
    "# have 100% parse rate but ~50% credit_relevant accuracy. Both metrics matter.\n",
    "\n",
    "if parsed_ok > 0:\n",
    "    # Compare parsed predictions to ground truth\n",
    "    field_metrics = {\n",
    "        \"credit_relevant\": {\"total\": 0, \"correct\": 0},\n",
    "        \"direction\": {\"total\": 0, \"correct\": 0},\n",
    "        \"signal_type\": {\"total\": 0, \"correct\": 0},\n",
    "        \"sector_wide\": {\"total\": 0, \"correct\": 0},\n",
    "        \"confidence\": {\"total\": 0, \"correct\": 0},\n",
    "    }\n",
    "\n",
    "    for pred, gt_dict in zip(parsed_results, sample):\n",
    "        if not pred.parse_ok:\n",
    "            continue\n",
    "\n",
    "        # Parse ground truth\n",
    "        gt = parse_model_output(gt_dict[\"output\"])\n",
    "        if not gt.parse_ok:\n",
    "            continue\n",
    "\n",
    "        # credit_relevant\n",
    "        field_metrics[\"credit_relevant\"][\"total\"] += 1\n",
    "        if pred.credit_relevant == gt.credit_relevant:\n",
    "            field_metrics[\"credit_relevant\"][\"correct\"] += 1\n",
    "\n",
    "        # Only compare other fields when both say credit-relevant\n",
    "        if pred.credit_relevant and gt.credit_relevant:\n",
    "            for field_name, pred_val, gt_val in [\n",
    "                (\"direction\", pred.direction, gt.direction),\n",
    "                (\"signal_type\", pred.signal_type, gt.signal_type),\n",
    "                (\"sector_wide\", pred.sector_wide, gt.sector_wide),\n",
    "                (\"confidence\", pred.confidence, gt.confidence),\n",
    "            ]:\n",
    "                field_metrics[field_name][\"total\"] += 1\n",
    "                if pred_val == gt_val:\n",
    "                    field_metrics[field_name][\"correct\"] += 1\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PER-FIELD ACCURACY (on successfully parsed outputs only)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  {'Field':20s}  {'Total':>6s}  {'Correct':>7s}  {'Accuracy':>8s}\")\n",
    "    print(\"-\" * 50)\n",
    "    for field_name, counts in field_metrics.items():\n",
    "        if counts[\"total\"] > 0:\n",
    "            acc = counts[\"correct\"] / counts[\"total\"]\n",
    "            print(f\"  {field_name:20s}  {counts['total']:>6d}  {counts['correct']:>7d}  {acc:>8.1%}\")\n",
    "        else:\n",
    "            print(f\"  {field_name:20s}  {'N/A':>6s}  {'N/A':>7s}  {'N/A':>8s}\")\n",
    "\n",
    "    # ğŸ“ KEY INSIGHT: For a base model, field accuracy tells you how much\n",
    "    # the model already \"understands\" about credit analysis vs how much\n",
    "    # it needs to learn. If credit_relevant accuracy is ~50%, the model is\n",
    "    # basically random. If direction accuracy is high but signal_type is low,\n",
    "    # the model gets the big picture but lacks domain vocabulary.\n",
    "else:\n",
    "    print(\"\\nNo successfully parsed outputs â€” cannot compute field accuracy.\")\n",
    "    print(\"This is expected if parse rate is very low.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Holdout-Aware Evaluation Scaffold\n",
    "\n",
    "ğŸ“ **This cell is a scaffold for post-training evaluation.** We define the per-entity\n",
    "holdout evaluation function here but DON'T run it on base model outputs (because the\n",
    "base model will be evaluated on train.jsonl samples, not entity_holdout.jsonl).\n",
    "\n",
    "After LoRA training (Phase 2.3), you'll run the fine-tuned model on `entity_holdout.jsonl`\n",
    "and use this function to get per-entity metrics.\n",
    "\n",
    "**Why per-entity?**\n",
    "- DHFL: 1,243 articles, 91% deterioration â€” a \"dumb\" model that always says deterioration scores well\n",
    "- Reliance Capital: 688 articles, 80% deterioration â€” similar but different crisis arc\n",
    "- Cholamandalam: 1,372 articles, 12% deterioration â€” the real test (false positive control)\n",
    "\n",
    "Aggregate holdout metrics hide Cholamandalam's failure behind DHFL's easy wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ SOURCE OF TRUTH: src/training/evaluate.py\n",
    "# This function is copied from the canonical evaluation module.\n",
    "# Post-training, import directly: from src.training.evaluate import evaluate_holdout_per_entity\n",
    "\n",
    "HOLDOUT_ENTITIES = {\n",
    "    \"DHFL\": {\"articles\": 1243, \"det_pct\": 91.3, \"purpose\": \"Crisis detection on unseen entity\"},\n",
    "    \"Reliance Capital\": {\"articles\": 688, \"det_pct\": 80.2, \"purpose\": \"Different crisis arc â€” generalization\"},\n",
    "    \"Cholamandalam\": {\"articles\": 1372, \"det_pct\": 11.9, \"purpose\": \"False positive control on stable entity\"},\n",
    "}\n",
    "\n",
    "\n",
    "def extract_entity_from_input(input_text):\n",
    "    \"\"\"Extract entity name from training input text.\n",
    "    Input format: 'Entity: DHFL\\nDate: 2019-06-01\\nTitle: ...\\nArticle: ...'\n",
    "    \"\"\"\n",
    "    for line in input_text.split(\"\\n\"):\n",
    "        if line.startswith(\"Entity:\"):\n",
    "            return line[len(\"Entity:\"):].strip()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def evaluate_holdout_per_entity(predictions, raw_outputs, ground_truth, entities):\n",
    "    \"\"\"Evaluate model performance per holdout entity.\n",
    "\n",
    "    Args:\n",
    "        predictions: list of ParsedOutput\n",
    "        raw_outputs: list of raw model output strings\n",
    "        ground_truth: list of dicts with 'output' field\n",
    "        entities: list of entity names (from extract_entity_from_input)\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping entity name â†’ metrics dict.\n",
    "    \"\"\"\n",
    "    entity_groups = defaultdict(list)\n",
    "    for i, entity in enumerate(entities):\n",
    "        entity_groups[entity].append(i)\n",
    "\n",
    "    results = {}\n",
    "    for entity, indices in entity_groups.items():\n",
    "        total = len(indices)\n",
    "        parsed_indices = [i for i in indices if predictions[i].parse_ok]\n",
    "        parsed = len(parsed_indices)\n",
    "        parse_rate = parsed / total if total > 0 else 0.0\n",
    "\n",
    "        # Failure modes\n",
    "        failure_modes = Counter()\n",
    "        for i in indices:\n",
    "            if not predictions[i].parse_ok:\n",
    "                failure_modes[classify_failure(raw_outputs[i], predictions[i])] += 1\n",
    "\n",
    "        # Per-field accuracy\n",
    "        cr_correct = dir_correct = st_correct = 0\n",
    "        dir_total = st_total = 0\n",
    "        det_tp = det_fp = det_fn = 0\n",
    "        imp_tp = imp_fp = imp_fn = 0\n",
    "\n",
    "        for i in parsed_indices:\n",
    "            gt = parse_model_output(ground_truth[i].get(\"output\", \"\"))\n",
    "            if not gt.parse_ok:\n",
    "                continue\n",
    "\n",
    "            if predictions[i].credit_relevant == gt.credit_relevant:\n",
    "                cr_correct += 1\n",
    "\n",
    "            if predictions[i].credit_relevant and gt.credit_relevant:\n",
    "                dir_total += 1\n",
    "                if predictions[i].direction == gt.direction:\n",
    "                    dir_correct += 1\n",
    "                st_total += 1\n",
    "                if predictions[i].signal_type == gt.signal_type:\n",
    "                    st_correct += 1\n",
    "\n",
    "            # Direction-specific\n",
    "            pred_det = predictions[i].credit_relevant and predictions[i].direction == \"Deterioration\"\n",
    "            gt_det = gt.credit_relevant and gt.direction == \"Deterioration\"\n",
    "            pred_imp = predictions[i].credit_relevant and predictions[i].direction == \"Improvement\"\n",
    "            gt_imp = gt.credit_relevant and gt.direction == \"Improvement\"\n",
    "\n",
    "            if pred_det and gt_det: det_tp += 1\n",
    "            elif pred_det and not gt_det: det_fp += 1\n",
    "            elif not pred_det and gt_det: det_fn += 1\n",
    "            if pred_imp and gt_imp: imp_tp += 1\n",
    "            elif pred_imp and not gt_imp: imp_fp += 1\n",
    "            elif not pred_imp and gt_imp: imp_fn += 1\n",
    "\n",
    "        results[entity] = {\n",
    "            \"total\": total,\n",
    "            \"parsed\": parsed,\n",
    "            \"parse_rate\": parse_rate,\n",
    "            \"cr_accuracy\": cr_correct / parsed if parsed > 0 else 0.0,\n",
    "            \"dir_accuracy\": dir_correct / dir_total if dir_total > 0 else 0.0,\n",
    "            \"st_accuracy\": st_correct / st_total if st_total > 0 else 0.0,\n",
    "            \"det_recall\": det_tp / (det_tp + det_fn) if (det_tp + det_fn) > 0 else 0.0,\n",
    "            \"det_precision\": det_tp / (det_tp + det_fp) if (det_tp + det_fp) > 0 else 0.0,\n",
    "            \"imp_recall\": imp_tp / (imp_tp + imp_fn) if (imp_tp + imp_fn) > 0 else 0.0,\n",
    "            \"imp_precision\": imp_tp / (imp_tp + imp_fp) if (imp_tp + imp_fp) > 0 else 0.0,\n",
    "            \"failure_modes\": dict(failure_modes),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_holdout_report(entity_metrics):\n",
    "    \"\"\"Print per-entity holdout evaluation table.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HOLDOUT ENTITY EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    for entity, em in entity_metrics.items():\n",
    "        info = HOLDOUT_ENTITIES.get(entity, {})\n",
    "        purpose = info.get(\"purpose\", \"\")\n",
    "        print(f\"\\n  {entity} ({em['total']} articles) â€” {purpose}\")\n",
    "        print(f\"  {'â”€' * 55}\")\n",
    "        print(f\"    Parse rate:         {em['parse_rate']:>6.1%}  ({em['parsed']}/{em['total']})\")\n",
    "        print(f\"    CR accuracy:        {em['cr_accuracy']:>6.1%}\")\n",
    "        print(f\"    Direction accuracy:  {em['dir_accuracy']:>6.1%}\")\n",
    "        print(f\"    Signal type acc:    {em['st_accuracy']:>6.1%}\")\n",
    "        print(f\"    Det. precision:     {em['det_precision']:>6.1%}\")\n",
    "        print(f\"    Det. recall:        {em['det_recall']:>6.1%}\")\n",
    "        print(f\"    Imp. precision:     {em['imp_precision']:>6.1%}\")\n",
    "        print(f\"    Imp. recall:        {em['imp_recall']:>6.1%}\")\n",
    "        if em[\"failure_modes\"]:\n",
    "            print(f\"    Failure modes: {em['failure_modes']}\")\n",
    "\n",
    "\n",
    "print(\"âœ… Holdout evaluation functions defined\")\n",
    "print()\n",
    "print(\"Holdout entities:\")\n",
    "for entity, info in HOLDOUT_ENTITIES.items():\n",
    "    print(f\"  {entity:20s}  {info['articles']:>5,d} articles  ({info['det_pct']:.1f}% det.)  {info['purpose']}\")\n",
    "print()\n",
    "print(\"To run holdout evaluation after training:\")\n",
    "print(\"  1. Load entity_holdout.jsonl\")\n",
    "print(\"  2. Run fine-tuned model on all 3,303 examples\")\n",
    "print(\"  3. entities = [extract_entity_from_input(ex['input']) for ex in holdout_data]\")\n",
    "print(\"  4. results = evaluate_holdout_per_entity(preds, raws, holdout_data, entities)\")\n",
    "print(\"  5. print_holdout_report(results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Decision â€” GO / INVESTIGATE / NO-GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2.1 â€” BASE MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"  Model:          {MODEL_NAME}\")\n",
    "print(f\"  Quantization:   4-bit NF4\")\n",
    "print(f\"  Sample size:    {total:,d} (stratified 50/50 CR)\")\n",
    "print(f\"  Inference time: {sum(inference_times)/60:.1f} min total\")\n",
    "print()\n",
    "\n",
    "# Parse rate summary\n",
    "print(f\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"  â”‚  PARSE SUCCESS RATE: {parse_rate:>6.1%}                â”‚\")\n",
    "print(f\"  â”‚  ({parsed_ok:,d} / {total:,d} examples)                  â”‚\")\n",
    "print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "\n",
    "# Decision logic\n",
    "if parse_rate >= 0.80:\n",
    "    decision = \"GO\"\n",
    "    print(\"  ğŸŸ¢ DECISION: GO\")\n",
    "    print(\"  Parse rate >= 80%. The base model already understands our output format.\")\n",
    "    print(\"  LoRA training will improve both format compliance and content accuracy.\")\n",
    "    print(\"  â†’ Proceed to Phase 2.2: LoRA training configuration.\")\n",
    "elif parse_rate >= 0.20:\n",
    "    decision = \"INVESTIGATE\"\n",
    "    print(\"  ğŸŸ¡ DECISION: INVESTIGATE\")\n",
    "    print(f\"  Parse rate {parse_rate:.1%} is between 20-80%. Review the failure taxonomy:\")\n",
    "    print()\n",
    "    if failure_counts:\n",
    "        top_failure = failure_counts.most_common(1)[0]\n",
    "        print(f\"  Top failure mode: {top_failure[0]} ({top_failure[1]} cases)\")\n",
    "    print()\n",
    "    print(\"  If mostly 'missing_end' or 'wrong_vocab':\")\n",
    "    print(\"    â†’ Fixable. Proceed to training â€” the model will learn these patterns.\")\n",
    "    print(\"  If mostly 'totally_unstructured' or 'refusal':\")\n",
    "    print(\"    â†’ Consider simplifying the format or adding examples to instruction.\")\n",
    "    print(\"  If mostly 'partial_format':\")\n",
    "    print(\"    â†’ The model partially gets it. Training should close the gap.\")\n",
    "else:\n",
    "    decision = \"NO-GO\"\n",
    "    print(\"  ğŸ”´ DECISION: NO-GO\")\n",
    "    print(f\"  Parse rate {parse_rate:.1%} < 20%. The format is too unfamiliar.\")\n",
    "    print(\"  Before training ($50-100), consider:\")\n",
    "    print(\"    1. Simplify output to fewer fields (just CREDIT_RELEVANT + DIRECTION)\")\n",
    "    print(\"    2. Switch to JSON output (models are more familiar with JSON)\")\n",
    "    print(\"    3. Add 2-3 output format examples to the instruction prompt\")\n",
    "    print(\"    4. Try LLaMA 3.1-8B or Mistral as alternative base models\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Summary for quick reference\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"â”‚  âœ… DONE: Phase 2.1 â€” Base model evaluation complete            â”‚\")\n",
    "print(f\"â”‚  ğŸ“Š Parse rate: {parse_rate:.1%} | Decision: {decision:12s}              â”‚\")\n",
    "if parsed_ok > 0 and field_metrics[\"credit_relevant\"][\"total\"] > 0:\n",
    "    cr_acc = field_metrics[\"credit_relevant\"][\"correct\"] / field_metrics[\"credit_relevant\"][\"total\"]\n",
    "    print(f\"â”‚  ğŸ“Š CR accuracy: {cr_acc:.1%} (on parsed outputs)                  â”‚\")\n",
    "print(f\"â”‚  ğŸ“ KEY INSIGHT: This is the performance FLOOR. Any fine-tuned   â”‚\")\n",
    "print(f\"â”‚     model must beat these numbers to justify the training cost.  â”‚\")\n",
    "print(f\"â”‚  â­ï¸  NEXT: Phase 2.2 â€” LoRA training config on Colab            â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ… GPU memory freed\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
