{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0.1 ‚Äî Colab Setup & FinRLlama Inference Test\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Sets up the project environment on Colab\n",
    "2. Verifies GPU access\n",
    "3. Logs into HuggingFace (needed for gated LLaMA model)\n",
    "4. Runs the FinRLlama prompt template on Qwen 2.5-3B to establish a BASELINE\n",
    "5. (Once approved) Runs the actual FinRLlama model for comparison\n",
    "\n",
    "**Why Colab?** Our 8GB Mac can't fit a 3B model in memory for inference. Colab Pro gives us an A100 with 40GB VRAM ‚Äî more than enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU\n",
    "\n",
    "First, make sure Colab gave us a GPU. Go to **Runtime ‚Üí Change runtime type ‚Üí T4 or A100**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì This cell checks what GPU Colab assigned us.\n",
    "# nvidia-smi is the NVIDIA command to see GPU info (like 'top' for your GPU).\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí select T4 or A100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Our Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì Clone our private repo into Colab's temporary filesystem.\n",
    "# Since you connected GitHub to Colab, authentication should work automatically.\n",
    "# If it asks for auth, use a GitHub Personal Access Token.\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/india-credit-signals\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/spiffler33/india-credit-signals.git {REPO_DIR}\n",
    "    print(f\"‚úÖ Cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "    print(f\"‚úÖ Pulled latest into {REPO_DIR}\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!git log --oneline -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "Colab comes with many packages pre-installed, but we need our specific versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì Install our project deps. We use pip here (not uv) because Colab's\n",
    "# environment is pre-built and uv venvs conflict with it.\n",
    "# The -q flag = quiet (less output noise).\n",
    "!pip install -q transformers>=4.48.0 peft>=0.14.0 datasets>=3.0.0 accelerate>=1.0.0 \\\n",
    "    bitsandbytes>=0.43.0 loguru>=0.7.0 httpx>=0.27.0 tenacity>=8.2.0 \\\n",
    "    polars>=1.0.0 pyyaml>=6.0\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed\")\n",
    "\n",
    "# Verify key imports\n",
    "import transformers, peft, datasets, accelerate, bitsandbytes\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
    "print(f\"torch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: HuggingFace Login\n",
    "\n",
    "We need this to access Meta's gated LLaMA model (base model for FinRLlama).\n",
    "\n",
    "**If your LLaMA access is still PENDING**, skip this ‚Äî Step 5 uses Qwen which doesn't need auth.\n",
    "\n",
    "To get your token: https://huggingface.co/settings/tokens ‚Üí New token ‚Üí Read access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì This stores your HuggingFace token so the transformers library\n",
    "# can download gated models. The token is saved in ~/.cache/huggingface/.\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option A: Use Colab's Secrets feature (recommended ‚Äî keeps token out of notebook)\n",
    "# Go to the üîë icon in Colab sidebar ‚Üí add secret named HF_TOKEN\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ Logged in via Colab Secrets\")\n",
    "except Exception:\n",
    "    # Option B: Paste token manually (will prompt you)\n",
    "    login()\n",
    "    print(\"‚úÖ Logged in manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Baseline Test ‚Äî Qwen 2.5-3B (no fine-tuning)\n",
    "\n",
    "We run the FinRLlama prompt template on a model that was NOT fine-tuned for finance.\n",
    "This is our **baseline** ‚Äî how well does a generic LLM do at scoring financial news?\n",
    "\n",
    "Later we compare: baseline Qwen ‚Üí fine-tuned FinRLlama ‚Üí our credit-risk model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# üéì Same prompt that FinRLlama was fine-tuned on (from their task2_signal.py).\n",
    "# An un-tuned model sees this for the first time ‚Äî it has to rely on general\n",
    "# language understanding, not specialized financial training.\n",
    "SIGNAL_PROMPT = \"\"\"\n",
    "Task: Analyze the following news headline about a stock and provide a sentiment score between -{signal_strength} and {signal_strength}, where:\n",
    "-{signal_strength} represents a highly negative sentiment, likely indicating a substantial decline in stock performance.\n",
    "-{threshold} represents a moderate negative sentiment, suggesting a slight potential decline in stock performance.\n",
    "0 represents neutral sentiment, indicating no significant impact on stock performance.\n",
    "{threshold} represents a moderate positive sentiment, indicating potential for slight stock growth.\n",
    "{signal_strength} represents a highly positive sentiment, indicating significant potential for stock appreciation.\n",
    "\n",
    "Consider the likely influence of market feedback from previous price movements and sentiment trends:\n",
    "How has the stock's price responded to similar news in the past?\n",
    "Does the headline align with prevailing market sentiment, or does it contradict current trends?\n",
    "How might this sentiment lead to a change in the stock's behavior, considering both historical price patterns and market expectations?\n",
    "\n",
    "Examples of sentiment scoring:\n",
    "\"Company X announces layoffs amidst economic downturn.\" Score: -8\n",
    "\"Company Y reports record revenue growth in Q1.\" Score: 7\n",
    "\"Market sees strong response to Company Z's new product release.\" Score: 5\n",
    "\n",
    "Do not provide any explanations or reasoning. Output only a single integer in the range of -{signal_strength} to {signal_strength} based on the sentiment of the news and its potential impact on stock performance.\n",
    "\n",
    "News headline: \"{news}\"\n",
    "\n",
    "Price Data: \"{prices}\"\n",
    "\n",
    "SENTIMENT SCORE:\n",
    "\"\"\"\n",
    "\n",
    "# Test cases: mix of US equity (in-domain) and Indian credit (out-of-domain)\n",
    "TEST_CASES = [\n",
    "    {\n",
    "        \"name\": \"Strong positive ‚Äî earnings beat\",\n",
    "        \"news\": \"Apple reports Q4 revenue of $94.9 billion, beating analyst estimates by 5%. iPhone sales surge 12% year-over-year driven by strong demand in emerging markets.\",\n",
    "        \"prices\": \"AAPL: Open=175.20, High=178.50, Low=174.80, Close=177.90, Volume=82M\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Strong negative ‚Äî fraud/governance\",\n",
    "        \"news\": \"SEC charges Wirecard executives with massive accounting fraud. Company files for insolvency after revealing 1.9 billion euros missing from accounts.\",\n",
    "        \"prices\": \"WDI: Open=104.50, High=104.50, Low=1.28, Close=1.28, Volume=350M\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ambiguous ‚Äî mixed signals\",\n",
    "        \"news\": \"Tesla announces 10% workforce reduction while simultaneously revealing record vehicle deliveries of 1.8 million units in 2023.\",\n",
    "        \"prices\": \"TSLA: Open=248.50, High=252.30, Low=245.10, Close=246.80, Volume=115M\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NBFC credit ‚Äî IL&FS crisis (out of domain)\",\n",
    "        \"news\": \"IL&FS defaults on Rs 1,000 crore commercial paper. RBI expresses concern about liquidity in NBFC sector. DHFL share price crashes 60% on contagion fears.\",\n",
    "        \"prices\": \"ILFS: Open=25.50, High=25.50, Low=12.20, Close=12.80, Volume=45M\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Regulatory ‚Äî RBI action (out of domain)\",\n",
    "        \"news\": \"RBI increases risk weights on NBFC lending by 25 basis points, citing rapid credit growth concerns. Banking stocks fall 2-3% across the board.\",\n",
    "        \"prices\": \"NIFTYBANK: Open=44250, High=44300, Low=43100, Close=43200, Volume=200M\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str):\n",
    "    \"\"\"Load a model onto GPU with appropriate settings.\"\"\"\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # üéì On Colab GPU we can load in fp16 directly onto CUDA.\n",
    "    # device_map=\"auto\" lets transformers figure out the best GPU placement.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    load_time = time.time() - t0\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"‚úÖ Loaded in {load_time:.1f}s | {param_count / 1e9:.2f}B params\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def run_test(model, tokenizer, model_label: str):\n",
    "    \"\"\"Run all test cases and print results.\"\"\"\n",
    "    signal_strength = 10\n",
    "    threshold = signal_strength // 3\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Results: {model_label}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    results = []\n",
    "    for i, case in enumerate(TEST_CASES, 1):\n",
    "        prompt = SIGNAL_PROMPT.format(\n",
    "            signal_strength=signal_strength,\n",
    "            threshold=threshold,\n",
    "            news=case[\"news\"],\n",
    "            prices=case[\"prices\"],\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        gen_time = time.time() - t0\n",
    "\n",
    "        new_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        match = re.search(r\"-?\\d+\", response)\n",
    "        score = int(match.group()) if match else None\n",
    "        direction = None\n",
    "        if score is not None:\n",
    "            direction = \"BULLISH\" if score >= threshold else \"BEARISH\" if score <= -threshold else \"NEUTRAL\"\n",
    "\n",
    "        results.append({\"name\": case[\"name\"], \"score\": score, \"direction\": direction, \"raw\": response})\n",
    "\n",
    "        print(f\"\\n[{i}/5] {case['name']}\")\n",
    "        print(f\"  Raw output: {repr(response)}\")\n",
    "        print(f\"  Score: {score} ‚Üí {direction}\" if score else f\"  Score: PARSE FAILED\")\n",
    "        print(f\"  Time: {gen_time:.2f}s\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì BASELINE: Qwen 2.5-3B-Instruct ‚Äî a general-purpose LLM, NOT fine-tuned for finance.\n",
    "# This shows us what a \"smart but untrained\" model does with the FinRLlama prompt.\n",
    "qwen_model, qwen_tokenizer = load_model(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "qwen_results = run_test(qwen_model, qwen_tokenizer, \"Qwen 2.5-3B (BASELINE ‚Äî no finance training)\")\n",
    "\n",
    "# Free GPU memory before loading next model\n",
    "del qwen_model, qwen_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: FinRLlama Test (skip if LLaMA access still pending)\n",
    "\n",
    "Run the SAME test cases on the fine-tuned FinRLlama model.\n",
    "This lets us compare: does fine-tuning on market feedback actually improve signal quality?\n",
    "\n",
    "**Skip this cell if your LLaMA access is still PENDING.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì FinRLlama: LLaMA 3.2-3B fine-tuned with RLMF (market feedback as reward).\n",
    "# This model was specifically trained to produce better sentiment scores.\n",
    "# Compare its outputs to the baseline Qwen above.\n",
    "try:\n",
    "    finrl_model, finrl_tokenizer = load_model(\"Arnav-Gr0ver/FinRLlama-3.2-3B-Instruct\")\n",
    "    finrl_results = run_test(finrl_model, finrl_tokenizer, \"FinRLlama 3.2-3B (RLMF fine-tuned)\")\n",
    "\n",
    "    del finrl_model, finrl_tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load FinRLlama: {e}\")\n",
    "    print(\"This is expected if LLaMA access is still PENDING.\")\n",
    "    print(\"Re-run this cell once Meta approves your access.\")\n",
    "    finrl_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Results\n",
    "\n",
    "Side-by-side comparison of baseline vs fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì Summary comparison table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Baseline vs Fine-tuned\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Test Case':<45} {'Qwen (base)':>12} {'FinRLlama':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, case in enumerate(TEST_CASES):\n",
    "    qwen_score = qwen_results[i][\"score\"] if qwen_results[i][\"score\"] is not None else \"FAIL\"\n",
    "    finrl_score = \"N/A\"\n",
    "    if finrl_results and finrl_results[i][\"score\"] is not None:\n",
    "        finrl_score = finrl_results[i][\"score\"]\n",
    "    elif finrl_results is None:\n",
    "        finrl_score = \"PENDING\"\n",
    "\n",
    "    print(f\"{case['name']:<45} {str(qwen_score):>12} {str(finrl_score):>12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéì KEY QUESTION: Do the scores make sense?\")\n",
    "print(\"   - Earnings beat should be strongly positive (+7 to +10)\")\n",
    "print(\"   - Fraud should be strongly negative (-8 to -10)\")\n",
    "print(\"   - Mixed signals should be near 0\")\n",
    "print(\"   - NBFC/RBI headlines are OUT OF DOMAIN ‚Äî interesting to see what happens\")\n",
    "print(\"   ‚Üí This is WHY we need to fine-tune: generic models don't understand credit risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What Just Happened\n",
    "\n",
    "```\n",
    "‚úÖ DONE: Phase 0.1 ‚Äî Baseline inference test on Colab\n",
    "üìä We now have baseline scores from an un-tuned model using the FinRLlama prompt\n",
    "üéì KEY INSIGHT: The generic model probably scores US equity headlines OK, but\n",
    "   struggles with Indian NBFC/credit headlines ‚Äî because it was never trained on them.\n",
    "   This gap is exactly what our fine-tuning will fix.\n",
    "‚è≠Ô∏è NEXT: Read FinGPT codebase (Phase 0.2), then start data collection (Phase 1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
